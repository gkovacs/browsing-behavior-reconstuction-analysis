{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# noexport\n",
    "\n",
    "import os\n",
    "os.system('export_notebook tmilib_base.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urlparse\n",
    "from glob import glob\n",
    "\n",
    "import os\n",
    "from os import path\n",
    "\n",
    "#import decompress_lzstring\n",
    "import pyximport\n",
    "pyximport.install()\n",
    "from decompress_lzstring_base64_cython import decompressFromBase64\n",
    "\n",
    "from memoized import memoized\n",
    "\n",
    "try:\n",
    "  import ujson as json\n",
    "except:\n",
    "  import json\n",
    "\n",
    "from collections import Counter\n",
    "import numpy\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "from operator import itemgetter\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmi_overrides = {\n",
    "  'basedir': None,\n",
    "}\n",
    "\n",
    "@memoized\n",
    "def get_basedir():\n",
    "  if tmi_overrides['basedir'] != None:\n",
    "    return tmi_overrides['basedir']\n",
    "  output = [x for x in glob('/home/gkovacs/tmi-data/local_*') if path.isfile(x + '/active')]\n",
    "  output.sort(reverse=True)\n",
    "  return output[0]\n",
    "  #return '/home/gkovacs/tmi-data/latest'\n",
    "\n",
    "@memoized\n",
    "def list_logfiles():\n",
    "  return glob(get_basedir() + '/logs_*.json')\n",
    "\n",
    "@memoized\n",
    "def list_mlogfiles():\n",
    "  return glob(get_basedir() + '/mlogs_*.json')\n",
    "\n",
    "@memoized\n",
    "def list_histfiles():\n",
    "  return glob(get_basedir() + '/hist_*.json')\n",
    "\n",
    "@memoized\n",
    "def list_users():\n",
    "  #return [filename_to_username(x) for x in list_logfiles()]\n",
    "  return list_users_with_log_and_mlog_and_hist()\n",
    "\n",
    "@memoized\n",
    "def list_users_with_hist():\n",
    "  return [filename_to_username(x) for x in list_histfiles()]\n",
    "\n",
    "@memoized\n",
    "def list_users_with_mlog():\n",
    "  return [filename_to_username(x) for x in list_mlogfiles()]\n",
    "\n",
    "@memoized\n",
    "def list_users_with_log():\n",
    "  return [filename_to_username(x) for x in list_logfiles()]\n",
    "\n",
    "@memoized\n",
    "def list_users_with_log_and_mlog():\n",
    "  users_with_mlog_set = set(list_users_with_mlog())\n",
    "  return [x for x in list_users_with_log() if x in users_with_mlog_set]\n",
    "\n",
    "@memoized\n",
    "def list_users_with_log_and_mlog_and_hist():\n",
    "  users_with_mlog_set = set(list_users_with_mlog())\n",
    "  users_with_hist_set = set(list_users_with_hist())\n",
    "  return [x for x in list_users_with_log() if x in users_with_mlog_set and x in users_with_hist_set]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@memoized\n",
    "def get_sdir():\n",
    "  #return get_basedir().replace('local_', 'sdir_')\n",
    "  return get_basedir().replace('tmi-data', 'tmi-sdir').replace('local_', 'sdir_')\n",
    "\n",
    "def ensure_sdir_exists():\n",
    "  sdir = get_sdir()\n",
    "  if path.exists(sdir):\n",
    "    return\n",
    "  os.makedirs(sdir)\n",
    "\n",
    "def ensure_sdir_subdir_exists(subdir):\n",
    "  ensure_sdir_exists()\n",
    "  sdir = get_sdir()\n",
    "  if path.exists(sdir + '/' + subdir):\n",
    "    return\n",
    "  os.makedirs(sdir + '/' + subdir)\n",
    "\n",
    "def sdir_path(filename):\n",
    "  return get_sdir() + '/' + filename\n",
    "\n",
    "def sdir_exists(filename):\n",
    "  return path.exists(sdir_path(filename))\n",
    "\n",
    "def sdir_open(filename, mode='r'):\n",
    "  return open(sdir_path(filename), mode)\n",
    "\n",
    "def sdir_loadjson(filename):\n",
    "  return json.load(sdir_open(filename))\n",
    "\n",
    "def sdir_loadjsonlines(filename):\n",
    "  jfile = sdir_open(filename)\n",
    "  for line in jfile:\n",
    "    yield json.loads(line)\n",
    "  #line = jfile.readline()\n",
    "  #while line != None:\n",
    "  #  yield json.loads(line)\n",
    "  #  line = jfile.readline()\n",
    "\n",
    "def sdir_dumpjson(filename, data):\n",
    "  ensure_sdir_exists()\n",
    "  return json.dump(data, sdir_open(filename, 'w'))\n",
    "\n",
    "def sdir_dumpjsonlines(filename, data):\n",
    "  ensure_sdir_exists()\n",
    "  outfile = sdir_open(filename, 'w')\n",
    "  for line in data:\n",
    "    outfile.write(json.dumps(line))\n",
    "    outfile.write('\\n')\n",
    "  outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dumpdir_path(filename):\n",
    "  return get_basedir() + '/' + filename\n",
    "\n",
    "def get_logfile_for_user(user):\n",
    "  return dumpdir_path('logs_' + user + '.json')\n",
    "\n",
    "def get_mlogfile_for_user(user):\n",
    "  return dumpdir_path('mlogs_' + user + '.json')\n",
    "\n",
    "def get_histfile_for_user(user):\n",
    "  return dumpdir_path('hist_' + user + '.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filename_to_username(filename):\n",
    "  if not filename.endswith('.json'):\n",
    "    raise Exception('expected filename to end with .json ' + filename)\n",
    "  filename = filename[:-5] # removes the .json\n",
    "  return filename.split('_')[-1] # returns part after the last _ which is the username\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def decompress_data_lzstring_base64(data):\n",
    "  data_type = type(data)\n",
    "  if data_type == unicode or data_type == str:\n",
    "    return json.loads(decompressFromBase64(data))\n",
    "  return data\n",
    "\n",
    "def uncompress_data_subfields(x):\n",
    "  if 'windows' in x:\n",
    "    data_type = type(x['windows'])\n",
    "    if data_type == unicode or data_type == str:\n",
    "      x['windows'] = json.loads(decompressFromBase64(x['windows']))\n",
    "  if 'data' in x:\n",
    "    data_type = type(x['data'])\n",
    "    if data_type == unicode or data_type == str:\n",
    "      x['data'] = json.loads(decompressFromBase64(x['data']))\n",
    "  return x\n",
    "\n",
    "def iterate_data_jsondata(data):\n",
    "  for x in data:\n",
    "    yield uncompress_data_subfields(x)\n",
    "\n",
    "def iterate_data(filename):\n",
    "  for x in json.load(open(filename)):\n",
    "    yield uncompress_data_subfields(x)\n",
    "\n",
    "def iterate_data_timesorted(filename):\n",
    "  alldata = json.load(open(filename))\n",
    "  alldata.sort(key=itemgetter('time'))\n",
    "  for x in alldata:\n",
    "    yield uncompress_data_subfields(x)\n",
    "\n",
    "def iterate_data_compressed_timesorted(filename):\n",
    "  alldata = json.load(open(filename))\n",
    "  alldata.sort(key=itemgetter('time'))\n",
    "  return alldata\n",
    "\n",
    "def iterate_data_compressed(filename):\n",
    "  return json.load(open(filename))\n",
    "\n",
    "def iterate_data_jsondata_reverse(data):\n",
    "  for x in reversed(data):\n",
    "    yield uncompress_data_subfields(x)\n",
    "\n",
    "def iterate_data_reverse(filename):\n",
    "  alldata = json.load(open(filename))\n",
    "  alldata.reverse()\n",
    "  for x in alldata:\n",
    "    yield uncompress_data_subfields(x)\n",
    "\n",
    "def iterate_data_compressed_reverse(filename):\n",
    "  alldata = json.load(open(filename))\n",
    "  alldata.reverse()\n",
    "  return alldata\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def url_to_domain(url):\n",
    "  return urlparse.urlparse(url).netloc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shuffled(l):\n",
    "  l = l[:]\n",
    "  random.shuffle(l)\n",
    "  return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def orderedMerge(*iterables, **kwargs):\n",
    "  # from http://stackoverflow.com/questions/464342/combining-two-sorted-lists-in-python\n",
    "  \"\"\"Take a list of ordered iterables; return as a single ordered generator.\n",
    "\n",
    "  @param key:     function, for each item return key value\n",
    "          (Hint: to sort descending, return negated key value)\n",
    "\n",
    "  @param unique:  boolean, return only first occurrence for each key value?\n",
    "  \"\"\"\n",
    "  key     = kwargs.get('key', (lambda x: x))\n",
    "  unique  = kwargs.get('unique', False)\n",
    "\n",
    "  _heapify       = heapq.heapify\n",
    "  _heapreplace   = heapq.heapreplace\n",
    "  _heappop       = heapq.heappop\n",
    "  _StopIteration = StopIteration\n",
    "\n",
    "  # preprocess iterators as heapqueue\n",
    "  h = []\n",
    "  for itnum, it in enumerate(map(iter, iterables)):\n",
    "    try:\n",
    "      next  = it.next\n",
    "      data   = next()\n",
    "      keyval = key(data)\n",
    "      h.append([keyval, itnum, data, next])\n",
    "    except _StopIteration:\n",
    "      pass\n",
    "  _heapify(h)\n",
    "\n",
    "  # process iterators in ascending key order\n",
    "  oldkeyval = None\n",
    "  while True:\n",
    "    try:\n",
    "      while True:\n",
    "        keyval, itnum, data, next = s = h[0]  # get smallest-key value\n",
    "                            # raises IndexError when h is empty\n",
    "        # if unique, skip duplicate keys\n",
    "        if unique and keyval==oldkeyval:\n",
    "          pass\n",
    "        else:\n",
    "          yield data\n",
    "          oldkeyval = keyval\n",
    "\n",
    "        # load replacement value from same iterator\n",
    "        s[2] = data = next()        # raises StopIteration when exhausted\n",
    "        s[0] = key(data)\n",
    "        _heapreplace(h, s)          # restore heap condition\n",
    "    except _StopIteration:\n",
    "      _heappop(h)                     # remove empty iterator\n",
    "    except IndexError:\n",
    "      return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

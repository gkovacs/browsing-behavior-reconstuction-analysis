{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# noexport\n",
    "\n",
    "import os\n",
    "os.system('export_notebook reconstruct_focus_times_v2.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tmilib import *\n",
    "\n",
    "#tmi_overrides['basedir'] = '/home/gkovacs/tmi-data/local_2016-03-30_16:39:38-07:00'\n",
    "\n",
    "from reconstruct_focus_times_common import *\n",
    "from sorted_collection import SortedCollection\n",
    "\n",
    "import sklearn\n",
    "import sklearn.svm\n",
    "import sklearn.linear_model\n",
    "import sklearn.ensemble\n",
    "import sklearn.naive_bayes\n",
    "import sklearn.tree\n",
    "\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_users = get_training_users()\n",
    "test_users = get_test_users()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print len(training_users)\n",
    "#print len(test_users)\n",
    "#print sorted(training_users)\n",
    "#print sorted(test_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#user = get_users_with_data()[0]\n",
    "#print user\n",
    "#user = '3a3FX1s9S6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for line in get_log_with_mlog_active_times_for_user(user):\n",
    "#  if line['evt'] != 'tab_updated':\n",
    "#    continue\n",
    "#  print line\n",
    "#  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tab_focus_times = get_tab_focus_times_for_user(user)\n",
    "#reconstructed_tab_focus_times = get_tab_focus_times_only_tab_updated_for_user(user)\n",
    "#print evalutate_tab_focus_reconstruction(tab_focus_times, reconstructed_tab_focus_times)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tab_focus_times = get_tab_focus_times_for_user(user)\n",
    "#reconstructed_tab_focus_times = get_tab_focus_times_only_tab_updated_urlchanged_for_user(user)\n",
    "#print evalutate_tab_focus_reconstruction(tab_focus_times, reconstructed_tab_focus_times)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tab_focus_times = get_tab_focus_times_for_user(user)\n",
    "#reconstructed_tab_focus_times = list(get_reconstruct_focus_times_baseline_for_user(user))\n",
    "#print evalutate_tab_focus_reconstruction(tab_focus_times, reconstructed_tab_focus_times)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# first: annotate the history visit items with whether the time between the two is majority activity or not\n",
    "#for visit in get_history_ordered_visits_for_user(user):\n",
    "#  print visit['transition']\n",
    "#  break\n",
    "\n",
    "def fraction_active_between_times(tab_focus_times_sortedcollection, target_start, target_end):\n",
    "  # between start and end, does tab_focus_times indicate that majority activity is occurring?\n",
    "  try:\n",
    "    item_before_start = tab_focus_times_sortedcollection.find_le(target_start)\n",
    "    item_before_start_idx = tab_focus_times_sortedcollection.index(item_before_start)\n",
    "  except:\n",
    "    item_before_start_idx = 0\n",
    "  # if have valueerror, make it 0\n",
    "  try:\n",
    "    item_after_end = tab_focus_times_sortedcollection.find_ge(target_end)\n",
    "    item_after_end_idx = tab_focus_times_sortedcollection.index(item_after_end)\n",
    "  except:\n",
    "    item_after_end_idx = len(tab_focus_times_sortedcollection) - 1\n",
    "  # if have valueerror, make it the last item in the list\n",
    "  time_span_duration = target_end - target_start\n",
    "  if time_span_duration <= 60*1000: # less than 60 seconds\n",
    "    return 1.0 # is this perhaps a cause of error?\n",
    "  if time_span_duration == 0:\n",
    "    return 1.0\n",
    "    #raise Exception('target_start == target_end both have value ' + str(target_start))\n",
    "    #return 0\n",
    "  if time_span_duration < 0:\n",
    "    raise Exception('target_end < target_start end=' + str(target_end) + ' start=' + str(target_start))\n",
    "  total_coverage = 0.0\n",
    "  for idx in range(item_before_start_idx, item_after_end_idx+1):\n",
    "    item = tab_focus_times_sortedcollection[idx]\n",
    "    if item['start'] == item['end']:\n",
    "      continue\n",
    "      #raise Exception('item has length 0')\n",
    "    if item['start'] > item['end']:\n",
    "      raise Exception('item start > end')\n",
    "    if item['end'] < target_start:\n",
    "      continue\n",
    "    if item['start'] > target_end:\n",
    "      continue\n",
    "    start = max(target_start, item['start'])\n",
    "    if start > target_end:\n",
    "      continue\n",
    "    end = min(target_end, item['end'])\n",
    "    if start > end:\n",
    "      raise Exception('start is greater than end start=' + str(start) + ' end=' + str(end))\n",
    "    start_percentage = (start - target_start) / float(time_span_duration)\n",
    "    if not 0.0 <= start_percentage <= 1.0:\n",
    "      continue\n",
    "    end_percentage = (end - target_start) / float(time_span_duration)\n",
    "    if not 0.0 <= end_percentage <= 1.0:\n",
    "      continue\n",
    "    total_coverage += (end_percentage - start_percentage)\n",
    "  return total_coverage\n",
    "\n",
    "def have_majority_activity(tab_focus_times_sortedcollection, target_start, target_end):\n",
    "  return fraction_active_between_times(tab_focus_times_sortedcollection, target_start, target_end) > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef get_domain_to_visit_durations():\\n  output = {}\\n  for user in training_users:\\n    tab_focus_times = get_tab_focus_times_for_user(user)\\n    for idx,item in enumerate(tab_focus_times):\\n      url = item['url']\\n      domain = url_to_domain(url)\\n      duration = item['end'] - item['start']\\n      if duration <= 0:\\n        continue\\n      if domain not in output:\\n        output[domain] = []\\n      output[domain].append(duration)\\n  return output\\nget_domain_to_visit_durations()\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def get_domain_to_visit_durations():\n",
    "  output = {}\n",
    "  for user in training_users:\n",
    "    tab_focus_times = get_tab_focus_times_for_user(user)\n",
    "    for idx,item in enumerate(tab_focus_times):\n",
    "      url = item['url']\n",
    "      domain = url_to_domain(url)\n",
    "      duration = item['end'] - item['start']\n",
    "      if duration <= 0:\n",
    "        continue\n",
    "      if domain not in output:\n",
    "        output[domain] = []\n",
    "      output[domain].append(duration)\n",
    "  return output\n",
    "get_domain_to_visit_durations()\n",
    "'''\n",
    "\n",
    "#def get_domain_to_canonical_visit_duration():\n",
    "#  output = {}\n",
    "#  for domain,visit_durations in get_domain_to_visit_durations().items():\n",
    "#    output[domain] = numpy.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#domain_to_visit_durations = get_domain_to_visit_durations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print numpy.mean([x for x in domain_to_visit_durations['www.youtube.com'] if x > 30000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def get_domain_to_duration_active_after_domain():\n",
    "#  output = {}\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntab_focus_times = get_tab_focus_times_for_user(user)\\n#tab_focus_times = get_tab_focus_times_only_tab_updated_urlchanged_for_user(user)\\ntab_focus_times_sortedcollection = SortedCollection(tab_focus_times, key=itemgetter('start'))\\n\\nordered_visits = get_history_ordered_visits_for_user(user)\\nordered_visits_len = len(ordered_visits)\\n\\nref_start_time = max(get_earliest_start_time(tab_focus_times), get_earliest_start_time(ordered_visits))\\nref_end_time = min(get_last_end_time(tab_focus_times), get_last_end_time(ordered_visits))\\n\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "tab_focus_times = get_tab_focus_times_for_user(user)\n",
    "#tab_focus_times = get_tab_focus_times_only_tab_updated_urlchanged_for_user(user)\n",
    "tab_focus_times_sortedcollection = SortedCollection(tab_focus_times, key=itemgetter('start'))\n",
    "\n",
    "ordered_visits = get_history_ordered_visits_for_user(user)\n",
    "ordered_visits_len = len(ordered_visits)\n",
    "\n",
    "ref_start_time = max(get_earliest_start_time(tab_focus_times), get_earliest_start_time(ordered_visits))\n",
    "ref_end_time = min(get_last_end_time(tab_focus_times), get_last_end_time(ordered_visits))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print (ref_end_time - ref_start_time)/(1000.0*3600*24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print ordered_visits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor idx,visit in enumerate(ordered_visits):\\n  if idx+1 == ordered_visits_len: # last visit\\n    continue\\n  next_visit = ordered_visits[idx + 1]\\n  visit_time = visit['visitTime']\\n  next_visit_time = next_visit['visitTime']\\n  if visit_time < ref_start_time:\\n    continue\\n  if next_visit_time > ref_end_time:\\n    continue\\n  if visit_time == next_visit_time:\\n    #print visit\\n    #print next_visit\\n    if visit['url'] != next_visit['url']:\\n      print visit\\n      print next_visit\\n      break\\n\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "for idx,visit in enumerate(ordered_visits):\n",
    "  if idx+1 == ordered_visits_len: # last visit\n",
    "    continue\n",
    "  next_visit = ordered_visits[idx + 1]\n",
    "  visit_time = visit['visitTime']\n",
    "  next_visit_time = next_visit['visitTime']\n",
    "  if visit_time < ref_start_time:\n",
    "    continue\n",
    "  if next_visit_time > ref_end_time:\n",
    "    continue\n",
    "  if visit_time == next_visit_time:\n",
    "    #print visit\n",
    "    #print next_visit\n",
    "    if visit['url'] != next_visit['url']:\n",
    "      print visit\n",
    "      print next_visit\n",
    "      break\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_tofill_dataset_from_user(user):\n",
    "  training_samples = []\n",
    "  training_labels = []\n",
    "  training_weights = []\n",
    "  from_domains = []\n",
    "  to_domains = []\n",
    "  #ordered_visits = get_history_ordered_visits_for_user(user)\n",
    "  #ordered_visits = exclude_bad_visits(ordered_visits)\n",
    "  #ordered_visits = get_idealized_history_from_logs_for_user(user)\n",
    "  ordered_visits = get_idealized_history_from_logs_urlchanged_for_user(user)\n",
    "  ordered_visits_len = len(ordered_visits)\n",
    "  tab_focus_times = get_tab_focus_times_for_user(user)\n",
    "  ref_start_time = max(get_earliest_start_time(tab_focus_times), get_earliest_start_time(ordered_visits))\n",
    "  ref_end_time = min(get_last_end_time(tab_focus_times), get_last_end_time(ordered_visits))\n",
    "  ref_start_time = max(ref_start_time, 1458371950000) # march 19th. may have had some data loss prior to that\n",
    "  ref_end_time = max(ref_end_time, 1458371950000)\n",
    "  tab_focus_times_sortedcollection = SortedCollection(tab_focus_times, key=itemgetter('start'))\n",
    "  for idx,visit in enumerate(ordered_visits):\n",
    "    if idx+1 == ordered_visits_len: # last visit, we probably should reconstruct this TODO\n",
    "      continue\n",
    "    next_visit = ordered_visits[idx + 1]\n",
    "    visit_time = visit['visitTime']\n",
    "    next_visit_time = next_visit['visitTime']\n",
    "    if visit_time < ref_start_time:\n",
    "      continue\n",
    "    if next_visit_time > ref_end_time:\n",
    "      continue\n",
    "    if visit_time >= next_visit_time:\n",
    "      continue\n",
    "    fraction_active = fraction_active_between_times(tab_focus_times_sortedcollection, visit_time, next_visit_time)\n",
    "    label = int(fraction_active > 0.5)\n",
    "    visit_gap = log(next_visit_time - visit_time)\n",
    "    weight = next_visit_time - visit_time\n",
    "    training_samples.append([visit_gap])\n",
    "    training_labels.append(label)\n",
    "    training_weights.append(weight) # should we try weight or log(weight)\n",
    "    from_domain = url_to_domain(visit['url'])\n",
    "    to_domain = url_to_domain(next_visit['url'])\n",
    "    from_domains.append(from_domain)\n",
    "    to_domains.append(to_domain)\n",
    "  return {\n",
    "    'samples': training_samples,\n",
    "    'labels': training_labels,\n",
    "    'weights': training_weights,\n",
    "    'fromdomains': from_domains,\n",
    "    'todomains': to_domains,\n",
    "  }\n",
    "\n",
    "def extract_tofill_dataset_for_users(users):\n",
    "  all_training_samples = []\n",
    "  all_training_labels = []\n",
    "  all_training_weights = []\n",
    "  all_from_domains = []\n",
    "  all_to_domains = []\n",
    "  for user in users:\n",
    "    data = extract_tofill_dataset_from_user(user)\n",
    "    all_training_samples.extend(data['samples'])\n",
    "    all_training_labels.extend(data['labels'])\n",
    "    all_training_weights.extend(data['weights'])\n",
    "    all_from_domains.extend(data['fromdomains'])\n",
    "    all_to_domains.extend(data['todomains'])\n",
    "  return {\n",
    "    'samples': all_training_samples,\n",
    "    'labels': all_training_labels,\n",
    "    'weights': all_training_weights,\n",
    "    'fromdomains': all_from_domains,\n",
    "    'todomains': all_to_domains,\n",
    "  }\n",
    "\n",
    "@jsonmemoized\n",
    "def extract_tofill_dataset_for_training():\n",
    "  return extract_tofill_dataset_for_users(training_users)\n",
    "\n",
    "@jsonmemoized\n",
    "def extract_tofill_dataset_for_test():\n",
    "  return extract_tofill_dataset_for_users(test_users)\n",
    "\n",
    "def train_tofill_classifier():\n",
    "  training_data = extract_tofill_dataset_for_training()\n",
    "  return train_classifier_on_data(training_data)\n",
    "\n",
    "def train_classifier_on_data(training_data):\n",
    "  #classifier = sklearn.naive_bayes.GaussianNB()\n",
    "  #classifier = sklearn.svm.LinearSVC()\n",
    "  #classifier = sklearn.linear_model.LogisticRegression(class_weight='balanced')\n",
    "  #classifier = sklearn.ensemble.RandomForestClassifier() #(class_weight='balanced')\n",
    "  #classifier = sklearn.tree.DecisionTreeClassifier(max_depth=1, class_weight='balanced')\n",
    "  classifier = sklearn.tree.DecisionTreeClassifier(max_depth=1)\n",
    "  #classifier.fit(numpy.array(training_data['samples']), numpy.array(training_data['labels']))\n",
    "  classifier.fit(numpy.array(training_data['samples']), numpy.array(training_data['labels']), numpy.array(training_data['weights']))\n",
    "  return classifier\n",
    "\n",
    "def train_tofill_classifier_v2():\n",
    "  training_data = extract_tofill_dataset_for_training()\n",
    "  def train_classifier_if_enough_data(key_to_data):\n",
    "    key_to_classifier = {}\n",
    "    for key,specialized_data in key_to_data.items():\n",
    "      positive_samples = specialized_data['labels'].count(1)\n",
    "      negative_samples = specialized_data['labels'].count(0)\n",
    "      if min(positive_samples, negative_samples) > 100: # enough to bother with a specialized classifier\n",
    "        key_to_classifier[key] = train_classifier_on_data(specialized_data)\n",
    "    return key_to_classifier\n",
    "  def extract_data_for_function(key_compute_function):\n",
    "    key_to_data = {}\n",
    "    for idx,sample in enumerate(training_data['samples']):\n",
    "      fromdomain = training_data['fromdomains'][idx]\n",
    "      todomain = training_data['todomains'][idx]\n",
    "      label = training_data['labels'][idx]\n",
    "      weight = training_data['weights'][idx]\n",
    "      key = key_compute_function(fromdomain, todomain)\n",
    "      if key not in key_to_data:\n",
    "        key_to_data[key] = {\n",
    "          'samples': [],\n",
    "          'labels': [],\n",
    "          'weights': [],\n",
    "        }\n",
    "      key_to_data[key]['samples'].append(sample)\n",
    "      key_to_data[key]['labels'].append(label)\n",
    "      key_to_data[key]['weights'].append(weight)\n",
    "    return key_to_data\n",
    "  def train_classifier_for_function(key_compute_function):\n",
    "    return train_classifier_if_enough_data(extract_data_for_function(key_compute_function))\n",
    "  general_classifier = train_classifier_on_data(training_data)\n",
    "  fromdomain_to_classifier = train_classifier_for_function(lambda f,t: f)\n",
    "  todomain_to_classifier = train_classifier_for_function(lambda f,t: t)\n",
    "  fromdomain_and_todomain_to_classifier = train_classifier_for_function(lambda f,t: f + ' to ' + t)\n",
    "  same_fromdomain_and_todomain_to_classifier = train_classifier_for_function(lambda f,t: f == t)\n",
    "  def fromdomain_same_compute_key(f, t):\n",
    "    if f == t:\n",
    "      return f + ' to same'\n",
    "    return f + ' to diff'\n",
    "  def todomain_same_compute_key(f, t):\n",
    "    if f == t:\n",
    "      return t + ' from same'\n",
    "    return t + ' from diff'\n",
    "  fromdomain_same_to_classifier = train_classifier_for_function(fromdomain_same_compute_key)\n",
    "  todomain_same_to_classifier = train_classifier_for_function(todomain_same_compute_key)\n",
    "  print 'fromdomain_and_todomain_to_classifier', len(fromdomain_and_todomain_to_classifier)\n",
    "  print 'todomain_to_classifier', len(todomain_to_classifier)\n",
    "  print 'fromdomain_to_classifier', len(fromdomain_to_classifier)\n",
    "  print 'same_fromdomain_and_todomain_to_classifier', len(same_fromdomain_and_todomain_to_classifier)\n",
    "  print 'fromdomain_same_to_classifier', len(fromdomain_same_to_classifier)\n",
    "  print 'todomain_same_to_classifier', len(todomain_same_to_classifier)\n",
    "  def classify_logtime_fromdomain_todomain(logtime, fromdomain, todomain):\n",
    "    fromdomain_and_todomain = fromdomain + ' to ' + todomain\n",
    "    same_fromdomain_and_todomain = fromdomain == todomain\n",
    "    fromdomain_same = fromdomain_same_compute_key(fromdomain, todomain)\n",
    "    todomain_same = todomain_same_compute_key(fromdomain, todomain)\n",
    "    if fromdomain_and_todomain in fromdomain_and_todomain_to_classifier:\n",
    "      cur_classifier = fromdomain_and_todomain_to_classifier[fromdomain_and_todomain]\n",
    "    elif fromdomain_same in fromdomain_to_classifier:\n",
    "      cur_classifier = fromdomain_same_to_classifier[fromdomain_same]\n",
    "    elif fromdomain in fromdomain_to_classifier:\n",
    "      cur_classifier = fromdomain_to_classifier[fromdomain]\n",
    "    elif todomain_same in todomain_same_to_classifier:\n",
    "      cur_classifier = todomain_same_to_classifier[todomain_same]\n",
    "    elif todomain in todomain_to_classifier:\n",
    "      cur_classifier = todomain_to_classifier[todomain]\n",
    "    elif same_fromdomain_and_todomain in same_fromdomain_and_todomain_to_classifier:\n",
    "      cur_classifier = same_fromdomain_and_todomain_to_classifier[same_fromdomain_and_todomain]\n",
    "    else:\n",
    "      cur_classifier = general_classifier\n",
    "    #return cur_classifier.predict([[logtime]])\n",
    "    threshold = cur_classifier.tree_.threshold[0]\n",
    "    return logtime < threshold\n",
    "  return classify_logtime_fromdomain_todomain\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfalse_samples = []\\ntraining_data = extract_tofill_dataset_for_training()\\nfor idx,label in enumerate(training_data['labels']):\\n  sample = training_data['samples'][idx]\\n  if label == False:\\n    false_samples.append(sample[0])\\n\\nprint numpy.histogram(false_samples)\\n\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "false_samples = []\n",
    "training_data = extract_tofill_dataset_for_training()\n",
    "for idx,label in enumerate(training_data['labels']):\n",
    "  sample = training_data['samples'][idx]\n",
    "  if label == False:\n",
    "    false_samples.append(sample[0])\n",
    "\n",
    "print numpy.histogram(false_samples)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print extract_tofill_dataset_from_user(user)['labels']\n",
    "#print len(extract_tofill_dataset_for_training()['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_predictions_with_classifier_on_dataset(cur_classifier, dataset):\n",
    "  output = []\n",
    "  for idx,sample in enumerate(dataset['samples']):\n",
    "    fromdomain = dataset['fromdomains'][idx]\n",
    "    todomain = dataset['todomains'][idx]\n",
    "    output.append(cur_classifier(sample[0], fromdomain, todomain))\n",
    "  return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fromdomain_and_todomain_to_classifier 26\n",
      "todomain_to_classifier 20\n",
      "fromdomain_to_classifier 19\n",
      "same_fromdomain_and_todomain_to_classifier 2\n",
      "fromdomain_same_to_classifier 23\n",
      "todomain_same_to_classifier 25\n"
     ]
    }
   ],
   "source": [
    "classifier = train_tofill_classifier_v2()\n",
    "#classifier = train_tofill_classifier()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.46      0.58     18185\n",
      "          1       0.99      1.00      0.99    739480\n",
      "\n",
      "avg / total       0.98      0.98      0.98    757665\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data = extract_tofill_dataset_for_test()\n",
    "test_predictions = make_predictions_with_classifier_on_dataset(classifier, test_data)\n",
    "#test_predictions = classifier.predict(test_data['samples'])\n",
    "print sklearn.metrics.classification_report(test_data['labels'], test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_code(tree, feature_names=['a', 'b', 'c', 'd', 'e', 'f']):\n",
    "  left      = tree.tree_.children_left\n",
    "  right     = tree.tree_.children_right\n",
    "  threshold = tree.tree_.threshold\n",
    "  features  = [feature_names[i] for i in tree.tree_.feature]\n",
    "  value = tree.tree_.value\n",
    "\n",
    "  def recurse(left, right, threshold, features, node):\n",
    "    if (threshold[node] != -2):\n",
    "      print \"if ( \" + features[node] + \" <= \" + str(threshold[node]) + \" ) {\"\n",
    "      if left[node] != -1:\n",
    "        recurse (left, right, threshold, features,left[node])\n",
    "      print \"} else {\"\n",
    "      if right[node] != -1:\n",
    "        recurse (left, right, threshold, features,right[node])\n",
    "      print \"}\"\n",
    "    else:\n",
    "      print \"return \" + str(value[node])\n",
    "\n",
    "  recurse(left, right, threshold, features, 0)\n",
    "\n",
    "#get_code(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sklearn.tree.export_graphviz(classifier, out_file='classifier.dot', feature_names=['a', 'b', 'c'])\n",
    "#os.system('dot -Tpng classifier.dot -o classifier.png')\n",
    "#from IPython.core.display import Image\n",
    "#Image('classifier.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print classifier.predict([[14]])\n",
    "#print classifier.predict([[13]])\n",
    "#classifier.predict([[log(6*60*1000.0)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def merge_contiguous_spans(visit_spans):\n",
    "  output = []\n",
    "  merged = {}\n",
    "  for span in visit_spans:\n",
    "    if 'url' not in merged:\n",
    "      merged = {k:v for k,v in span.items()}\n",
    "      continue\n",
    "    if merged['url'] == span['url']: # merge this current span into the merged one\n",
    "      if span['start'] <= merged['end']:\n",
    "        merged['end'] = max(merged['end'], span['end'])\n",
    "        merged['active'] = max(merged['active'], span['active'])\n",
    "        continue\n",
    "      else: # end of current merged segment, start of new one\n",
    "        output.append(merged)\n",
    "        merged = {k:v for k,v in span.items()}\n",
    "    else: # end of current merged segment, start of new one\n",
    "      output.append(merged)\n",
    "      merged = {k:v for k,v in span.items()}\n",
    "  if 'url' in merged:\n",
    "    output.append(merged)\n",
    "  return output\n",
    "\n",
    "#print merge_contiguous_spans([{'url': 'a', 'start': 0, 'end': 2}, {'url': 'a', 'start': 5, 'end': 7}, {'url': 'b', 'start': 10, 'end': 13}])\n",
    "#print merge_contiguous_spans([{'url': 'a', 'start': 0, 'end': 2}, {'url': 'a', 'start': 2, 'end': 7}, {'url': 'b', 'start': 10, 'end': 13}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reconstruct_for_user_v2(user):\n",
    "  ordered_visits = get_history_ordered_visits_for_user(user)\n",
    "  ordered_visits = exclude_bad_visits(ordered_visits)\n",
    "  output = []\n",
    "  ordered_visits_len = len(ordered_visits)\n",
    "  for idx,visit in enumerate(ordered_visits):\n",
    "    if idx+1 == ordered_visits_len: # last visit, TODO needs to be reconstructed\n",
    "      continue\n",
    "    next_visit = ordered_visits[idx+1]\n",
    "    visit_time = visit['visitTime']\n",
    "    next_visit_time = next_visit['visitTime']\n",
    "    url = visit['url']\n",
    "    next_url = next_visit['url']\n",
    "    time_difference = next_visit_time - visit_time\n",
    "    if time_difference <= 0:\n",
    "      continue\n",
    "    log_time_difference = log(time_difference)\n",
    "    #extend_to_next_visit = classifier.predict([[log_time_difference]])[0]\n",
    "    #extend_to_next_visit = log_time_difference < 13.5336971283\n",
    "    #extend_to_next_visit = log_time_difference < classifier.tree_.threshold[0]\n",
    "    fromdomain = url_to_domain(url)\n",
    "    todomain = url_to_domain(next_url)\n",
    "    extend_to_next_visit = classifier(log_time_difference, fromdomain, todomain)\n",
    "    end_time = min(visit_time + 3.0*60*1000.0, next_visit_time)\n",
    "    if extend_to_next_visit:\n",
    "      end_time = next_visit_time\n",
    "    output.append({'url': url, 'next_url': next_url, 'start': visit_time, 'active': visit_time, 'end': end_time})\n",
    "\n",
    "  output = merge_contiguous_spans(output)\n",
    "  return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reconstruct_for_user_perfect_predictions(user):\n",
    "  ordered_visits = get_history_ordered_visits_for_user(user)\n",
    "  ordered_visits = exclude_bad_visits(ordered_visits)\n",
    "  tab_focus_times = get_tab_focus_times_for_user(user)\n",
    "  tab_focus_times_sortedcollection = SortedCollection(tab_focus_times, key=itemgetter('start'))\n",
    "  output = []\n",
    "  ordered_visits_len = len(ordered_visits)\n",
    "  for idx,visit in enumerate(ordered_visits):\n",
    "    if idx+1 == ordered_visits_len: # last visit, TODO needs to be reconstructed\n",
    "      continue\n",
    "    next_visit = ordered_visits[idx+1]\n",
    "    visit_time = visit['visitTime']\n",
    "    next_visit_time = next_visit['visitTime']\n",
    "    url = visit['url']\n",
    "    next_url = next_visit['url']\n",
    "    time_difference = next_visit_time - visit_time\n",
    "    if time_difference <= 0:\n",
    "      continue\n",
    "    fraction_active = fraction_active_between_times(tab_focus_times_sortedcollection, visit_time, next_visit_time)\n",
    "    end_time = min(visit_time + 60*1000.0, next_visit_time)\n",
    "    if fraction_active > 0.5:\n",
    "      end_time = next_visit_time\n",
    "    output.append({'url': url, 'start': visit_time, 'active': visit_time, 'end': end_time})\n",
    "\n",
    "  output = merge_contiguous_spans(output)\n",
    "  return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "def find_end_of_matching_span(tab_focus_times_sortedcollection, url, visit_time):\n",
    "  try:\n",
    "    item = tab_focus_times_sortedcollection.find_le(visit_time)\n",
    "    if item['end'] >= visit_time:\n",
    "      return item['end']\n",
    "    item = tab_focus_times_sortedcollection.find_ge(visit_time)\n",
    "    if item['end'] >= visit_time:\n",
    "      return item['end']\n",
    "  except:\n",
    "    return None\n",
    "'''\n",
    "\n",
    "def find_best_matching_span(tab_focus_times_sortedcollection, url, visit_time):\n",
    "  try:\n",
    "    item = tab_focus_times_sortedcollection.find_le(visit_time)\n",
    "    idx = tab_focus_times_sortedcollection.index(item)\n",
    "  except:\n",
    "    return None\n",
    "  best_matching_span = None\n",
    "  while idx < len(tab_focus_times_sortedcollection):\n",
    "    span = tab_focus_times_sortedcollection[idx]\n",
    "    if span['url'] == url and span['start'] <= visit_time+5000 and span['end'] > visit_time:\n",
    "      return span\n",
    "    idx += 1\n",
    "\n",
    "def find_end_of_best_matching_span(tab_focus_times_sortedcollection, url, visit_time):\n",
    "  span = find_best_matching_span(tab_focus_times_sortedcollection, url, visit_time)\n",
    "  if span == None:\n",
    "    return None\n",
    "  return span['end']\n",
    "\n",
    "\n",
    "# this one sadly does not appear to work correctly\n",
    "def reconstruct_for_user_perfect_predictions_v2(user):\n",
    "  ordered_visits = get_history_ordered_visits_for_user(user)\n",
    "  ordered_visits = exclude_bad_visits(ordered_visits)\n",
    "  tab_focus_times = get_tab_focus_times_for_user(user)\n",
    "  tab_focus_times_sortedcollection = SortedCollection(tab_focus_times, key=itemgetter('start'))\n",
    "  output = []\n",
    "  ordered_visits_len = len(ordered_visits)\n",
    "  for idx,visit in enumerate(ordered_visits):\n",
    "    if idx+1 == ordered_visits_len: # last visit, TODO needs to be reconstructed\n",
    "      continue\n",
    "    next_visit = ordered_visits[idx+1]\n",
    "    visit_time = visit['visitTime']\n",
    "    next_visit_time = next_visit['visitTime']\n",
    "    url = visit['url']\n",
    "    next_url = next_visit['url']\n",
    "    time_difference = next_visit_time - visit_time\n",
    "    if time_difference <= 0:\n",
    "      continue\n",
    "    end_time_proposed = find_end_of_best_matching_span(tab_focus_times_sortedcollection, url, visit_time)\n",
    "    if end_time_proposed != None:\n",
    "      end_time = end_time_proposed\n",
    "    else:\n",
    "      fraction_active = fraction_active_between_times(tab_focus_times_sortedcollection, visit_time, next_visit_time)\n",
    "      end_time = min(visit_time + 60*1000.0, next_visit_time)\n",
    "      if fraction_active > 0.5:\n",
    "        end_time = next_visit_time\n",
    "    output.append({'url': url, 'start': visit_time, 'active': visit_time, 'end': end_time})\n",
    "\n",
    "  output = merge_contiguous_spans(output)\n",
    "  return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reconstruct_for_user_naive(minutes_threshold):\n",
    "  def returned_function(user):\n",
    "    ordered_visits = get_history_ordered_visits_for_user(user)\n",
    "    ordered_visits = exclude_bad_visits(ordered_visits)\n",
    "    output = []\n",
    "    ordered_visits_len = len(ordered_visits)\n",
    "    for idx,visit in enumerate(ordered_visits):\n",
    "      if idx+1 == ordered_visits_len: # last visit, TODO needs to be reconstructed\n",
    "        continue\n",
    "      next_visit = ordered_visits[idx+1]\n",
    "      visit_time = visit['visitTime']\n",
    "      next_visit_time = next_visit['visitTime']\n",
    "      url = visit['url']\n",
    "      next_url = next_visit['url']\n",
    "      time_difference = next_visit_time - visit_time\n",
    "      if time_difference <= 0:\n",
    "        continue\n",
    "      #log_time_difference = log(time_difference)\n",
    "      #extend_to_next_visit = classifier.predict([[log_time_difference]])[0]\n",
    "      #extend_to_next_visit = log_time_difference < 13.5336971283\n",
    "      #extend_to_next_visit = log_time_difference < classifier.tree_.threshold[0]\n",
    "      #fromdomain = url_to_domain(url)\n",
    "      #todomain = url_to_domain(next_url)\n",
    "      #extend_to_next_visit = classifier(log_time_difference, fromdomain, todomain)\n",
    "      end_time = min(visit_time + minutes_threshold*60*1000.0, next_visit_time)\n",
    "      #if extend_to_next_visit:\n",
    "      #  end_time = next_visit_time\n",
    "      output.append({'url': url, 'start': visit_time, 'active': visit_time, 'end': end_time})\n",
    "\n",
    "    output = merge_contiguous_spans(output)\n",
    "    return output\n",
    "  return returned_function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reconstruct_for_user_v3(user):\n",
    "  #ordered_visits = get_history_ordered_visits_for_user(user)\n",
    "  #ordered_visits = exclude_bad_visits(ordered_visits)\n",
    "  ordered_visits = get_idealized_history_from_logs_urlchanged_for_user(user)\n",
    "  #ordered_visits = get_idealized_history_from_logs_for_user(user)\n",
    "  output = []\n",
    "  ordered_visits_len = len(ordered_visits)\n",
    "  for idx,visit in enumerate(ordered_visits):\n",
    "    if idx+1 == ordered_visits_len: # last visit, TODO needs to be reconstructed\n",
    "      continue\n",
    "    next_visit = ordered_visits[idx+1]\n",
    "    visit_time = visit['visitTime']\n",
    "    next_visit_time = next_visit['visitTime']\n",
    "    url = visit['url']\n",
    "    next_url = next_visit['url']\n",
    "    time_difference = next_visit_time - visit_time\n",
    "    if time_difference <= 0:\n",
    "      continue\n",
    "    log_time_difference = log(time_difference)\n",
    "    #extend_to_next_visit = classifier.predict([[log_time_difference]])[0]\n",
    "    extend_to_next_visit = log_time_difference < 13.5336971283\n",
    "    #extend_to_next_visit = log_time_difference < classifier.tree_.threshold[0]\n",
    "    end_time = min(visit_time + 60*1000.0, next_visit_time)\n",
    "    if extend_to_next_visit:\n",
    "      end_time = next_visit_time\n",
    "    output.append({'url': url, 'start': visit_time, 'active': visit_time, 'end': end_time})\n",
    "\n",
    "  output = merge_contiguous_spans(output)\n",
    "  return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate_reconstruction_algorithm_for_user(user, reconstruction_algorithm):\n",
    "  #user = '3a3FX1s9S6'\n",
    "  reconstructed_tab_focus_times = reconstruction_algorithm(user)\n",
    "  tab_focus_times = get_tab_focus_times_for_user(user)\n",
    "  #reconstructed_tab_focus_times = tab_focus_times = get_tab_focus_times_for_user(user)\n",
    "  #reconstructed_tab_focus_times = list(get_reconstruct_focus_times_baseline_for_user(user))\n",
    "  return evalutate_tab_focus_reconstruction_fast(tab_focus_times, reconstructed_tab_focus_times)\n",
    "\n",
    "  #ref_start_time = max(get_earliest_start_time(tab_focus_times), get_earliest_start_time(reconstructed_tab_focus_times))\n",
    "  #ref_end_time = min(get_last_end_time(tab_focus_times), get_last_end_time(reconstructed_tab_focus_times))\n",
    "  #evaluated_reconstructed_tab_focus_times = ignore_all_before_start_or_after_end(reconstructed_tab_focus_times, ref_start_time, ref_end_time)\n",
    "  #evaluated_tab_focus_times = ignore_all_before_start_or_after_end(tab_focus_times, ref_start_time, ref_end_time)\n",
    "\n",
    "  #return evalutate_tab_focus_reconstruction(evaluated_tab_focus_times, evaluated_reconstructed_tab_focus_times)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate_reconstruction_algorithm(reconstruction_algorithm):\n",
    "  overall_evaluation_results = Counter()\n",
    "  #for user in test_users:\n",
    "  for user in training_users[:10]:\n",
    "    evaluation_results = evaluate_reconstruction_algorithm_for_user(user, reconstruction_algorithm)\n",
    "    for k,v in evaluation_results.items():\n",
    "      overall_evaluation_results[k] += v\n",
    "  return overall_evaluation_results\n",
    "\n",
    "def evaluate_reconstruction_algorithm_test(reconstruction_algorithm):\n",
    "  overall_evaluation_results = Counter()\n",
    "  for user in test_users:\n",
    "    evaluation_results = evaluate_reconstruction_algorithm_for_user(user, reconstruction_algorithm)\n",
    "    for k,v in evaluation_results.items():\n",
    "      overall_evaluation_results[k] += v\n",
    "  return overall_evaluation_results\n",
    "\n",
    "def evaluate_reconstruction_algorithm_train(reconstruction_algorithm):\n",
    "  overall_evaluation_results = Counter()\n",
    "  for user in training_users:\n",
    "    evaluation_results = evaluate_reconstruction_algorithm_for_user(user, reconstruction_algorithm)\n",
    "    for k,v in evaluation_results.items():\n",
    "      overall_evaluation_results[k] += v\n",
    "  return overall_evaluation_results\n",
    "\n",
    "\n",
    "def sumfields(d, *args):\n",
    "  return sum(d.get(x, 0) for x in args)\n",
    "\n",
    "def print_evaluation_results(results):\n",
    "  ref_active_time = float(sumfields(results, 'correct_url', 'ref_active_but_rec_inactive', 'incorrect_domain', 'correct_domain', 'incorrect_domain_next_url_is_none', 'incorrect_domain_ref_equals_next_domain', 'incorrect_domain_other', 'nexturl_correct', 'nextdomain_correct'))\n",
    "  correct_span = float(sumfields(results, 'correct_url', 'incorrect_domain', 'correct_domain', 'incorrect_domain_next_url_is_none', 'incorrect_domain_ref_equals_next_domain', 'incorrect_domain_other', 'nexturl_correct', 'nextdomain_correct'))\n",
    "  print 'correct span', correct_span, 'which is', correct_span/ref_active_time, 'of ref_active_time'\n",
    "  correct_url = results['correct_url']\n",
    "  ref_inactive_but_rec_active = results['ref_inactive_but_rec_active']\n",
    "  ref_active_but_rec_inactive = results['ref_active_but_rec_inactive']\n",
    "  precision = correct_span/(correct_span + ref_inactive_but_rec_active)\n",
    "  recall = correct_span/(correct_span + ref_active_but_rec_inactive)\n",
    "  f1 = 2*(precision*recall)/(precision+recall)\n",
    "  tp = correct_span\n",
    "  tn = results['both_inactive']\n",
    "  fp = results['ref_inactive_but_rec_active']\n",
    "  fn = results['ref_active_but_rec_inactive']\n",
    "  sum4 = float(sum([tp, tn, fp, fn]))\n",
    "  print '=== span reconstruction evaluation ==='\n",
    "  print 'precision', precision\n",
    "  print 'recall', recall\n",
    "  print 'f1', f1\n",
    "  print 'tp', tp/sum4\n",
    "  print 'tn', tn/sum4\n",
    "  print 'fp', fp/sum4\n",
    "  print 'fn', fn/sum4\n",
    "  #print 'fp ref_inactive_but_rec_active', ref_inactive_but_rec_active/ref_active_time, 'of ref_active_time'\n",
    "  print '=== url reconstruction evaluation ==='\n",
    "  print 'correct url', correct_url/ref_active_time, 'of ref_active_time', correct_url/correct_span, 'of correct_span'\n",
    "  nexturl_correct = results.get('nexturl_correct', 0)\n",
    "  correct_domain = results['correct_domain']\n",
    "  nextdomain_correct = results.get('nextdomain_correct', 0)\n",
    "  incorrect_domain = float(sumfields(results, 'incorrect_domain', 'incorrect_domain_next_url_is_none', 'incorrect_domain_ref_equals_next_domain', 'incorrect_domain_other'))\n",
    "  print 'nexturl_correct', nexturl_correct/ref_active_time, 'of ref_active_time', nexturl_correct/correct_span, 'of correct span'\n",
    "  print 'correct_domain', correct_domain/ref_active_time, 'of ref_active_time', correct_domain/correct_span, 'of correct span'\n",
    "  print 'nextdomain_correct', nextdomain_correct/ref_active_time, 'of ref_active_time', nextdomain_correct/correct_span, 'of correct span'\n",
    "  print 'incorrect_domain', incorrect_domain/ref_active_time, 'of ref_active_time', incorrect_domain/correct_span, 'of correct span'\n",
    "  print results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print_evaluation_results(evaluate_reconstruction_algorithm_for_user('3a3FX1s9S6', reconstruct_for_user_naive))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print_evaluation_results(evaluate_reconstruction_algorithm(reconstruct_for_user_naive))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print_evaluation_results(evaluate_reconstruction_algorithm_test(reconstruct_for_user_naive(0.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print_evaluation_results(evaluate_reconstruction_algorithm_test(reconstruct_for_user_naive(1.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print_evaluation_results(evaluate_reconstruction_algorithm_test(reconstruct_for_user_naive(2.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print_evaluation_results(evaluate_reconstruction_algorithm_test(reconstruct_for_user_naive(3.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print_evaluation_results(evaluate_reconstruction_algorithm_test(reconstruct_for_user_naive(4.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print_evaluation_results(evaluate_reconstruction_algorithm_test(reconstruct_for_user_naive(5.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print_evaluation_results(evaluate_reconstruction_algorithm_test(reconstruct_for_user_naive(6.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print_evaluation_results(evaluate_reconstruction_algorithm_test(reconstruct_for_user_naive(7.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print_evaluation_results(evaluate_reconstruction_algorithm_test(reconstruct_for_user_naive(8.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#print_evaluation_results(evaluate_reconstruction_algorithm_test(reconstruct_for_user_naive(9.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print_evaluation_results(evaluate_reconstruction_algorithm_test(reconstruct_for_user_naive(10.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print_evaluation_results(evaluate_reconstruction_algorithm_for_user('3a3FX1s9S6', reconstruct_for_user_perfect_predictions_v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print_evaluation_results(evaluate_reconstruction_algorithm_for_user('3a3FX1s9S6', reconstruct_for_user_perfect_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print_evaluation_results(evaluate_reconstruction_algorithm(reconstruct_for_user_perfect_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct span 12390782431.2 which is 0.870518663509 of ref_active_time\n",
      "=== span reconstruction evaluation ===\n",
      "precision 0.887264008782\n",
      "recall 0.870518663509\n",
      "f1 0.878811574696\n",
      "tp 0.1421793968\n",
      "tn 0.818607425529\n",
      "fp 0.0180653504147\n",
      "fn 0.0211478272561\n",
      "=== url reconstruction evaluation ===\n",
      "correct url 0.595153884121 of ref_active_time 0.683677339807 of correct_span\n",
      "nexturl_correct 0.0 of ref_active_time 0.0 of correct span\n",
      "correct_domain 0.0622658055458 of ref_active_time 0.0715272493926 of correct span\n",
      "nextdomain_correct 0.0 of ref_active_time 0.0 of correct span\n",
      "incorrect_domain 0.213098973843 of ref_active_time 0.2447954108 of correct span\n",
      "Counter({'both_inactive': 71340761985.4519, 'correct_url': 8471297170.722656, 'incorrect_domain_other': 2245061316.1828613, 'ref_active_but_rec_inactive': 1843010536.8388672, 'ref_inactive_but_rec_active': 1574375975.5039062, 'correct_domain': 886278585.1293945, 'incorrect_domain_ref_equals_next_domain': 689509119.4919434, 'incorrect_domain_next_url_is_none': 98636239.71826172})\n"
     ]
    }
   ],
   "source": [
    "print_evaluation_results(evaluate_reconstruction_algorithm_test(reconstruct_for_user_perfect_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct span 41367255.1348 which is 0.90218274039 of ref_active_time\n",
      "=== span reconstruction evaluation ===\n",
      "precision 0.476742811326\n",
      "recall 0.90218274039\n",
      "f1 0.62383228079\n",
      "tp 0.0330653300055\n",
      "tn 0.927058215518\n",
      "fp 0.0362914158541\n",
      "fn 0.00358503862293\n",
      "=== url reconstruction evaluation ===\n",
      "correct url 0.612520779495 of ref_active_time 0.678932052314 of correct_span\n",
      "nexturl_correct 0.0271578070733 of ref_active_time 0.0301023349899 of correct span\n",
      "correct_domain 0.0578075235223 of ref_active_time 0.0640751822599 of correct span\n",
      "nextdomain_correct 0.0424050252548 of ref_active_time 0.0470027006241 of correct span\n",
      "incorrect_domain 0.162291605044 of ref_active_time 0.179887729812 of correct span\n",
      "Counter({'both_inactive': 1159820685.8896484, 'ref_inactive_but_rec_active': 45403335.11230469, 'correct_url': 28085555.427246094, 'incorrect_domain_other': 6548363.990722656, 'ref_active_but_rec_inactive': 4485157.334228516, 'correct_domain': 2650614.4123535156, 'nextdomain_correct': 1944372.7087402344, 'nexturl_correct': 1245250.9716796875, 'incorrect_domain_next_url_is_none': 699343.6669921875, 'incorrect_domain_ref_equals_next_domain': 193753.95703125})\n"
     ]
    }
   ],
   "source": [
    "print_evaluation_results(evaluate_reconstruction_algorithm_for_user('3a3FX1s9S6', reconstruct_for_user_v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct span 1667990795.75 which is 0.74494274503 of ref_active_time\n",
      "=== span reconstruction evaluation ===\n",
      "precision 0.76841069901\n",
      "recall 0.74494274503\n",
      "f1 0.756494760276\n",
      "tp 0.141641617358\n",
      "tn 0.767173412697\n",
      "fp 0.0426889984708\n",
      "fn 0.0484959714741\n",
      "=== url reconstruction evaluation ===\n",
      "correct url 0.455347558918 of ref_active_time 0.61125175318 of correct_span\n",
      "nexturl_correct 0.0177223018932 of ref_active_time 0.0237901530171 of correct span\n",
      "correct_domain 0.0524876947196 of ref_active_time 0.0704586964163 of correct span\n",
      "nextdomain_correct 0.0370945831463 of ref_active_time 0.0497952136507 of correct span\n",
      "incorrect_domain 0.182290606353 of ref_active_time 0.244704183736 of correct span\n",
      "Counter({'both_inactive': 9034337612.039307, 'correct_url': 1019562298.1896973, 'ref_active_but_rec_inactive': 571095102.9709473, 'ref_inactive_but_rec_active': 502711405.4296875, 'incorrect_domain_other': 347809248.9338379, 'correct_domain': 117524457.10302734, 'nextdomain_correct': 83057958.04174805, 'incorrect_domain_next_url_is_none': 40307761.6953125, 'nexturl_correct': 39681756.26196289, 'incorrect_domain_ref_equals_next_domain': 20047315.524658203})\n"
     ]
    }
   ],
   "source": [
    "print_evaluation_results(evaluate_reconstruction_algorithm(reconstruct_for_user_v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print_evaluation_results(evaluate_reconstruction_algorithm_test(reconstruct_for_user_v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct span 10360007312.0 which is 0.72783254396 of ref_active_time\n",
      "=== span reconstruction evaluation ===\n",
      "precision 0.746274604706\n",
      "recall 0.72783254396\n",
      "f1 0.736938213111\n",
      "tp 0.118874937284\n",
      "tn 0.796256473024\n",
      "fp 0.0404162090775\n",
      "fn 0.0444523806144\n",
      "=== url reconstruction evaluation ===\n",
      "correct url 0.500926082329 of ref_active_time 0.688243589115 of correct_span\n",
      "nexturl_correct 0.0134997931514 of ref_active_time 0.0185479383458 of correct span\n",
      "correct_domain 0.0504557689828 of ref_active_time 0.0693233208676 of correct span\n",
      "nextdomain_correct 0.0378389101391 of ref_active_time 0.051988483413 of correct span\n",
      "incorrect_domain 0.125111989357 of ref_active_time 0.171896668259 of correct span\n",
      "Counter({'both_inactive': 69394130261.79248, 'correct_url': 7130208615.686523, 'ref_active_but_rec_inactive': 3874046108.644287, 'ref_inactive_but_rec_active': 3522291839.916748, 'incorrect_domain_other': 1486239466.4279785, 'correct_domain': 718190111.0822754, 'nextdomain_correct': 538601068.3000488, 'nexturl_correct': 192156776.88549805, 'incorrect_domain_next_url_is_none': 170093613.7878418, 'incorrect_domain_ref_equals_next_domain': 124517659.85888672})\n"
     ]
    }
   ],
   "source": [
    "print_evaluation_results(evaluate_reconstruction_algorithm_test(reconstruct_for_user_v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct span 11741558821.3 which is 0.775500185521 of ref_active_time\n",
      "=== span reconstruction evaluation ===\n",
      "precision 0.759956381554\n",
      "recall 0.775500185521\n",
      "f1 0.767649606664\n",
      "tp 0.134572911301\n",
      "tn 0.783962658569\n",
      "fp 0.0425068719176\n",
      "fn 0.0389575582124\n",
      "=== url reconstruction evaluation ===\n",
      "correct url 0.503847686564 of ref_active_time 0.649706726022 of correct_span\n",
      "nexturl_correct 0.0151611193394 of ref_active_time 0.0195501169729 of correct span\n",
      "correct_domain 0.0543200415262 of ref_active_time 0.0700451689637 of correct span\n",
      "nextdomain_correct 0.0412076094143 of ref_active_time 0.053136814386 of correct span\n",
      "incorrect_domain 0.160963728676 of ref_active_time 0.207561173655 of correct span\n",
      "Counter({'both_inactive': 68401163207.73779, 'correct_url': 7628569740.172119, 'ref_inactive_but_rec_active': 3708747415.0786133, 'ref_active_but_rec_inactive': 3399067887.1411133, 'incorrect_domain_other': 2088549353.6931152, 'correct_domain': 822439471.5339355, 'nextdomain_correct': 623909031.6884766, 'nexturl_correct': 229548848.3996582, 'incorrect_domain_next_url_is_none': 202784897.8479004, 'incorrect_domain_ref_equals_next_domain': 145757477.94458008})\n"
     ]
    }
   ],
   "source": [
    "print_evaluation_results(evaluate_reconstruction_algorithm_train(reconstruct_for_user_v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print_evaluation_results(evaluate_reconstruction_algorithm_for_user('3a3FX1s9S6', reconstruct_for_user_v3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print_evaluation_results(evaluate_reconstruction_algorithm(reconstruct_for_user_v3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print_evaluation_results(evaluate_reconstruction_algorithm_test(reconstruct_for_user_v3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nmax_weight = 0\\nsamples_for_max_weight = []\\nnum_printed = 0\\nfor idx in range(len(test_predictions)):\\n  ref = test_data['labels'][idx]\\n  pred = test_predictions[idx]\\n  weight = test_data['weights'][idx]\\n  samples = test_data['samples'][idx]\\n  if ref == 0 and pred == 1:\\n    #max_weight = max(max_weight, weight)\\n    if weight > max_weight:\\n      max_weight = weight\\n      samples_for_max_weight = samples\\n    #print weight\\n    #print samples\\n    num_printed += 1\\n    #if num_printed >= 100:\\n    #  break\\n  \\nprint max_weight\\nprint samples_for_max_weight\\n\""
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "max_weight = 0\n",
    "samples_for_max_weight = []\n",
    "num_printed = 0\n",
    "for idx in range(len(test_predictions)):\n",
    "  ref = test_data['labels'][idx]\n",
    "  pred = test_predictions[idx]\n",
    "  weight = test_data['weights'][idx]\n",
    "  samples = test_data['samples'][idx]\n",
    "  if ref == 0 and pred == 1:\n",
    "    #max_weight = max(max_weight, weight)\n",
    "    if weight > max_weight:\n",
    "      max_weight = weight\n",
    "      samples_for_max_weight = samples\n",
    "    #print weight\n",
    "    #print samples\n",
    "    num_printed += 1\n",
    "    #if num_printed >= 100:\n",
    "    #  break\n",
    "  \n",
    "print max_weight\n",
    "print samples_for_max_weight\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print classifier.predict([[log(60*1000)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print classifier.predict([[19.010298647]])\n",
    "#print classifier.predict([[15.010298647]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#a = [{'time': x} for x in [5,3,7,9,2]]\n",
    "#b = SortedCollection(a, key=itemgetter('time'))\n",
    "#print b.find_le(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tab_focus_times = get_tab_focus_times_for_user(user)\n",
    "#reconstructed_tab_focus_times = tab_focus_times = get_tab_focus_times_for_user(user)\n",
    "#reconstructed_tab_focus_times = list(get_reconstruct_focus_times_baseline_for_user(user))\n",
    "#print evalutate_tab_focus_reconstruction(tab_focus_times, reconstructed_tab_focus_times)\n",
    "\n",
    "#for visit in get_history_ordered_visits_for_user(user):\n",
    "#  print visit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

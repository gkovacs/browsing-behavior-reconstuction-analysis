{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# noexport\n",
    "\n",
    "import os\n",
    "os.system('export_notebook reconstruct_spans_persecond_h2o.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import sys\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/gkovacs/ve/lib/python2.7/site-packages/IPython/core/ultratb.py\", line 1118, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/gkovacs/ve/lib/python2.7/site-packages/IPython/core/ultratb.py\", line 300, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/gkovacs/ve/lib/python2.7/site-packages/IPython/core/ultratb.py\", line 345, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/gkovacs/.locinst/lib/python2.7/inspect.py\", line 1049, in getinnerframes\n",
      "    framelist.append((tb.tb_frame,) + getframeinfo(tb, context))\n",
      "  File \"/home/gkovacs/.locinst/lib/python2.7/inspect.py\", line 1009, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/gkovacs/.locinst/lib/python2.7/inspect.py\", line 454, in getsourcefile\n",
      "    if hasattr(getmodule(object, filename), '__loader__'):\n",
      "  File \"/home/gkovacs/.locinst/lib/python2.7/inspect.py\", line 500, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/home/gkovacs/ve/lib/python2.7/posixpath.py\", line 375, in realpath\n",
      "    path, ok = _joinrealpath('', filename, {})\n",
      "  File \"/home/gkovacs/ve/lib/python2.7/posixpath.py\", line 400, in _joinrealpath\n",
      "    if not islink(newpath):\n",
      "  File \"/home/gkovacs/ve/lib/python2.7/posixpath.py\", line 135, in islink\n",
      "    st = os.lstat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "\n",
      "Unfortunately, your original traceback can not be constructed.\n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "string index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/gkovacs/ve/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_code\u001b[1;34m(self, code_obj, result)\u001b[0m\n\u001b[0;32m   2900\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2901\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_in_exec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2902\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowtraceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2903\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2904\u001b[0m             \u001b[0moutflag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/gkovacs/ve/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mshowtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only)\u001b[0m\n\u001b[0;32m   1828\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1829\u001b[0m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[1;32m-> 1830\u001b[1;33m                                             value, tb, tb_offset=tb_offset)\n\u001b[0m\u001b[0;32m   1831\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1832\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_showtraceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/gkovacs/ve/lib/python2.7/site-packages/IPython/core/ultratb.pyc\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1390\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1391\u001b[0m         return FormattedTB.structured_traceback(\n\u001b[1;32m-> 1392\u001b[1;33m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[0m\u001b[0;32m   1393\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1394\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/gkovacs/ve/lib/python2.7/site-packages/IPython/core/ultratb.pyc\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1298\u001b[0m             \u001b[1;31m# Verbose modes need a full traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1299\u001b[0m             return VerboseTB.structured_traceback(\n\u001b[1;32m-> 1300\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1301\u001b[0m             )\n\u001b[0;32m   1302\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/gkovacs/ve/lib/python2.7/site-packages/IPython/core/ultratb.pyc\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1182\u001b[0m                 \u001b[0mstructured_traceback_parts\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mformatted_exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1183\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1184\u001b[1;33m             \u001b[0mstructured_traceback_parts\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mformatted_exception\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mstructured_traceback_parts\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: string index out of range"
     ]
    }
   ],
   "source": [
    "from tmilib import *\n",
    "\n",
    "from reconstruct_focus_times_common import *\n",
    "from sorted_collection import SortedCollection\n",
    "from rescuetime_utils import *\n",
    "\n",
    "import sklearn\n",
    "import sklearn.svm\n",
    "import sklearn.linear_model\n",
    "import sklearn.ensemble\n",
    "import sklearn.naive_bayes\n",
    "import sklearn.tree\n",
    "\n",
    "from math import log\n",
    "\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "import cPickle as pickle\n",
    "\n",
    "import h2o\n",
    "import h2o.grid\n",
    "h2o.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_users = get_training_users()\n",
    "test_users = get_test_users()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print domain_to_id('www.facebook.com')\n",
    "#print id_to_domain(29708)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def extract_secondlevel_dataset_from_users(users):\n",
    "  all_labels = []\n",
    "  all_sinceprev = []\n",
    "  all_tonext = []\n",
    "  all_fromdomain = []\n",
    "  all_todomain = []\n",
    "  for user in users:\n",
    "    data = extract_secondlevel_dataset_from_user(user)\n",
    "    all_labels.extend(data['label'])\n",
    "    all_sinceprev.extend(data['sinceprev'])\n",
    "    all_tonext.extend(data['tonext'])\n",
    "    all_fromdomain.extend(data['fromdomain'])\n",
    "    all_todomain.extend(data['todomain'])\n",
    "  return {\n",
    "    'label': all_labels,\n",
    "    'sinceprev': all_sinceprev,\n",
    "    'tonext': all_tonext,\n",
    "    'fromdomain': all_fromdomain,\n",
    "    'todomain': all_todomain,\n",
    "  }\n",
    "\n",
    "@jsonmemoized\n",
    "def extract_secondlevel_dataset_for_training():\n",
    "  return extract_secondlevel_dataset_from_users(training_users)\n",
    "\n",
    "@jsonmemoized\n",
    "def extract_secondlevel_dataset_for_test():\n",
    "  return extract_secondlevel_dataset_from_users(test_users)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_tensecondlevel_dataset_from_users(users):\n",
    "  all_labels = []\n",
    "  all_sinceprev = []\n",
    "  all_tonext = []\n",
    "  all_fromdomain = []\n",
    "  all_todomain = []\n",
    "  for user in users:\n",
    "    data = extract_secondlevel_dataset_from_user(user, True)\n",
    "    all_labels.extend(data['label'])\n",
    "    all_sinceprev.extend(data['sinceprev'])\n",
    "    all_tonext.extend(data['tonext'])\n",
    "    all_fromdomain.extend(data['fromdomain'])\n",
    "    all_todomain.extend(data['todomain'])\n",
    "  return {\n",
    "    'label': all_labels,\n",
    "    'sinceprev': all_sinceprev,\n",
    "    'tonext': all_tonext,\n",
    "    'fromdomain': all_fromdomain,\n",
    "    'todomain': all_todomain,\n",
    "  }\n",
    "\n",
    "@jsonmemoized\n",
    "def extract_tensecondlevel_dataset_for_training():\n",
    "  return extract_tensecondlevel_dataset_from_users(training_users)\n",
    "\n",
    "@jsonmemoized\n",
    "def extract_tensecondlevel_dataset_for_test():\n",
    "  return extract_tensecondlevel_dataset_from_users(test_users)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#a=extract_secondlevel_training_data_from_user(training_users[0])\n",
    "#print 'extracting dataset for training'\n",
    "#extract_secondlevel_dataset_for_training()\n",
    "#extract_tensecondlevel_dataset_for_training()\n",
    "#print 'extracting dataset for test'\n",
    "#extract_secondlevel_dataset_for_test()\n",
    "#extract_tensecondlevel_dataset_for_test()\n",
    "#print 'extraction done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print zipkeys({'a':[3,4,5], 'b':[6,7,8]}, 'a', 'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "@memoized\n",
    "def total_usage_of_domains_in_training():\n",
    "  return sum_values_in_list_of_dict([get_domain_to_time_spent_for_user(user)for user in training_users])\n",
    "\n",
    "@memoized\n",
    "def top_n_domains_by_usage(n=10):\n",
    "  domain_to_usage = total_usage_of_domains_in_training()\n",
    "  return [x[0] for x in sorted(domain_to_usage.items(), key=itemgetter(1), reverse=True)[:n]]\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print top_n_domains_by_visits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print top_n_domains_by_usage(10)\n",
    "#print top_n_domains_by_usage(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print domain_to_id('newtab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "def dataset_to_feature_vectors(dataset):\n",
    "  topdomains = numpy.array([domain_to_id(x) for x in top_n_domains_by_visits(20)])\n",
    "  num_features = 3 + 2*len(topdomains) + 2*len(get_rescuetime_productivity_levels())\n",
    "  output = [[0]*num_features for x in xrange(len(dataset['sinceprev']))]\n",
    "  #output = numpy.zeros((len(dataset['sinceprev']), num_features), dtype=object) # object instead of float, so we can have floats and ints\n",
    "  for idx,sinceprev,tonext,fromdomain,todomain in zipkeys_idx(dataset, 'sinceprev', 'tonext', 'fromdomain', 'todomain'):\n",
    "    cur = output[idx]\n",
    "    cur[0] = sinceprev\n",
    "    cur[1] = tonext\n",
    "    cur[2] = int(fromdomain == todomain)\n",
    "    feature_num = 3\n",
    "    for domain_idx,domain in enumerate(topdomains):\n",
    "      cur[feature_num+domain_idx] = int(fromdomain == domain)\n",
    "    feature_num += len(topdomains)\n",
    "    for domain_idx,domain in enumerate(topdomains):\n",
    "      cur[feature_num+domain_idx] = int(todomain == domain)\n",
    "    feature_num += len(topdomains)\n",
    "    fromdomain_name = id_to_domain(fromdomain)\n",
    "    todomain_name = id_to_domain(todomain)\n",
    "    fromdomain_productivity = domain_to_productivity(fromdomain_name)\n",
    "    todomain_productivity = domain_to_productivity(todomain_name)\n",
    "    for productivity_idx,productivity in enumerate(get_rescuetime_productivity_levels()):\n",
    "      cur[feature_num+productivity_idx] = int(fromdomain_productivity == productivity)\n",
    "    feature_num += len(get_rescuetime_productivity_levels())\n",
    "    for productivity_idx,productivity in enumerate(get_rescuetime_productivity_levels()):\n",
    "      cur[feature_num+productivity_idx] = int(todomain_productivity == productivity)\n",
    "    feature_num += len(get_rescuetime_productivity_levels())\n",
    "  return output\n",
    "'''\n",
    "\n",
    "def remove_cached_features():\n",
    "  os.remove('get_test_feature_vector.msgpack')\n",
    "  os.remove('get_training_feature_vector.msgpack')\n",
    "  #os.remove('get_test_labels.msgpack')\n",
    "  #os.remove('get_training_labels.msgpack')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#remove_cached_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "@msgpackmemoized\n",
    "def get_training_feature_vector():\n",
    "  return dataset_to_feature_vectors(extract_tensecondlevel_dataset_for_training())\n",
    "\n",
    "@msgpackmemoized\n",
    "def get_training_labels():\n",
    "  return extract_tensecondlevel_dataset_for_training()['label']\n",
    "\n",
    "@msgpackmemoized\n",
    "def get_test_feature_vector():\n",
    "  return dataset_to_feature_vectors(extract_tensecondlevel_dataset_for_test())\n",
    "\n",
    "@msgpackmemoized\n",
    "def get_test_labels():\n",
    "  return extract_tensecondlevel_dataset_for_test()['label']\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "def train_classifier_on_data(training_data):\n",
    "  #classifier = sklearn.tree.DecisionTreeClassifier(max_depth=2) # .71 on test\n",
    "  #classifier = sklearn.tree.DecisionTreeClassifier(max_depth=1)\n",
    "  #classifier = sklearn.tree.DecisionTreeClassifier()\n",
    "  #classifier = sklearn.naive_bayes.GaussianNB() # .73 on test\n",
    "  #classifier = sklearn.svm.LinearSVC()\n",
    "  #classifier = sklearn.linear_model.SGDClassifier(class_weight='balanced')\n",
    "  classifier = sklearn.linear_model.SGDClassifier(loss='modified_huber') # .73 on test\n",
    "  classifier.fit(dataset_to_feature_vectors(training_data), numpy.array(training_data['label']))\n",
    "  return classifier\n",
    "\n",
    "def get_classifier():\n",
    "  return train_classifier_on_data(extract_tensecondlevel_dataset_for_training())\n",
    "'''\n",
    "\n",
    "global_feature_filter = '11100000000000000000000000000000000000000000000000000'\n",
    "\n",
    "def get_feature_filter():\n",
    "  return global_feature_filter\n",
    "  #return '11000000000000000000000000000000000000000000000000000'\n",
    "  #return '11111111111111111111111111111111111111111111111111111'\n",
    "  #return '11100000000000000000000000000000000000000000000000000'\n",
    "  #selected_features = [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "  #selected_features = [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "  #selected_features = [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "  #selected_features = [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "  #selected_features = [1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "  #selected_features = [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "  #selected_features = [1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
    "  #selected_features = [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "  #selected_features = [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0]\n",
    "  #selected_features = [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
    "  #selected_features = [True,False,True,False,False,False,False,True,True,False,True,False]\n",
    "  return ''.join(map(str, selected_features))\n",
    "\n",
    "'''\n",
    "def get_filtered_features():\n",
    "  selected_features = get_feature_filter()\n",
    "  selected_features_str = ''.join(map(str, selected_features))\n",
    "  selected_features_filename = 'filtered_features_' + selected_features_str + '.msgpack'\n",
    "  if path.exists(selected_features_filename):\n",
    "    #return numpy.loadtxt()\n",
    "    #return json.load(open(selected_features_filename))\n",
    "    return msgpack.load(open(selected_features_filename))\n",
    "  output = filter_features(get_training_feature_vector())\n",
    "  msgpack.dump(output, open(selected_features_filename, 'w'))\n",
    "  #json.dump(output, open(selected_features_filename, 'w'))\n",
    "  return output\n",
    "'''\n",
    "\n",
    "'''\n",
    "def get_filtered_features_train():\n",
    "  selected_features = get_feature_filter()\n",
    "  selected_features_str = ''.join(map(str, selected_features))\n",
    "  selected_features_filename = 'features_train_' + selected_features_str + '.msgpack'\n",
    "  if path.exists(selected_features_filename):\n",
    "    return msgpack.load(open(selected_features_filename))\n",
    "  output = dataset_to_feature_vectors(extract_tensecondlevel_dataset_for_training(), selected_features)\n",
    "  msgpack.dump(output, open(selected_features_filename, 'w'))\n",
    "  return output\n",
    "\n",
    "def get_filtered_features_test():\n",
    "  selected_features = get_feature_filter()\n",
    "  selected_features_str = ''.join(map(str, selected_features))\n",
    "  selected_features_filename = 'features_test_' + selected_features_str + '.msgpack'\n",
    "  if path.exists(selected_features_filename):\n",
    "    return msgpack.load(open(selected_features_filename))\n",
    "  output = dataset_to_feature_vectors(extract_tensecondlevel_dataset_for_test(), selected_features)\n",
    "  msgpack.dump(output, open(selected_features_filename, 'w'))\n",
    "  return output\n",
    "'''\n",
    "\n",
    "def get_filtered_features_train():\n",
    "  selected_features = get_feature_filter()\n",
    "  return get_feature_vector_for_tensecondlevel_train(selected_features)\n",
    "\n",
    "def get_filtered_features_test():\n",
    "  selected_features = get_feature_filter()\n",
    "  return get_feature_vector_for_tensecondlevel_test(selected_features)\n",
    "\n",
    "def get_test_labels():\n",
    "  return get_labels_for_tensecondlevel_test()\n",
    "\n",
    "def get_training_labels():\n",
    "  return get_labels_for_tensecondlevel_train()\n",
    "\n",
    "#def get_labels_for_user(user):\n",
    "#  return get_tensecondlevel_activespan_labels_for_user(user)\n",
    "\n",
    "'''\n",
    "# we normally want to use the tensecond level ones\n",
    "def get_filtered_features_train():\n",
    "  selected_features = get_feature_filter()\n",
    "  return get_feature_vector_for_secondlevel_train(selected_features)\n",
    "\n",
    "def get_filtered_features_test():\n",
    "  selected_features = get_feature_filter()\n",
    "  return get_feature_vector_for_secondlevel_test(selected_features)\n",
    "\n",
    "def get_test_labels():\n",
    "  return get_labels_for_secondlevel_test()\n",
    "\n",
    "def get_training_labels():\n",
    "  return get_labels_for_secondlevel_train()\n",
    "\n",
    "def get_labels_for_user(user):\n",
    "  return get_secondlevel_activespan_labels_for_user(user)\n",
    "'''\n",
    "\n",
    "\n",
    "'''\n",
    "def filter_features(arr):\n",
    "  # from get_selected_features()\n",
    "  selected_features = get_feature_filter()\n",
    "  #selected_feature_idx = [i for i,x in enumerate(selected_features) if x]\n",
    "  #return arr[:,selected_feature_idx]\n",
    "  output = []\n",
    "  for line in arr:\n",
    "    output.append([line[i] for i,x in enumerate(selected_features) if x])\n",
    "    #output.append([x for i,x in enumerate(line) if selected_features[i]])\n",
    "  return output\n",
    "'''\n",
    "\n",
    "#classifier_algorithm = sklearn.ensemble.AdaBoostClassifier\n",
    "train_dataset = sdir_path('catdata_train_tensecond.csv')\n",
    "#classifier_algorithm = h2o.estimators.H2ORandomForestEstimator\n",
    "classifier_algorithm = h2o.estimators.H2OGradientBoostingEstimator\n",
    "\n",
    "def get_classifier():\n",
    "  #classifier = sklearn.naive_bayes.GaussianNB()\n",
    "  #classifier = sklearn.linear_model.SGDClassifier(loss='modified_huber') # .73 on test\n",
    "  #classifier = sklearn.linear_model.SGDClassifier()\n",
    "  #classifier = sklearn.ensemble.RandomForestClassifier()\n",
    "  #classifier = sklearn.ensemble.AdaBoostClassifier()\n",
    "  #classifier = sklearn.ensemble.GradientBoostingClassifier()\n",
    "  #classifier = classifier_algorithm()\n",
    "  classifier = classifier_algorithm() #h2o.estimators.H2ORandomForestEstimator(binomial_double_trees=True)\n",
    "  #classifier.fit(get_filtered_features_train(), get_training_labels())\n",
    "  #classifier.train(x=get_filtered_features_train(), y=get_training_labels())\n",
    "  training_data = h2o.import_file(train_dataset)\n",
    "  test_data = h2o.import_file(train_dataset.replace('train', 'test'))\n",
    "  classifier.train(x=training_data.columns[1:], y=training_data.columns[0], training_frame=training_data, validation_frame=test_data)\n",
    "  return classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print len(get_filtered_features_train())\n",
    "#print len(get_training_labels())\n",
    "#print len(get_filtered_features_test())\n",
    "#print len(get_test_labels())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#a=get_filtered_features_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#b=get_test_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print len(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print len(get_feature_filter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print len(get_training_feature_vector()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "def make_predictions_with_classifier_on_dataset(classifier, dataset):\n",
    "  return classifier.predict(dataset_to_feature_vectors(dataset))\n",
    "\n",
    "def make_proba_predictions_with_classifier_on_dataset(classifier, dataset):\n",
    "  return [x[1] for x in classifier.predict_proba(dataset_to_feature_vectors(dataset))]\n",
    "\n",
    "def evaluate_classifier(classifier):\n",
    "  test_predictions = make_predictions_with_classifier_on_dataset(classifier, test_data)\n",
    "  print sklearn.metrics.classification_report(test_data['label'], test_predictions)\n",
    "'''\n",
    "\n",
    "def make_predictions_with_classifier_on_test(classifier):\n",
    "  #return classifier.predict(filter_features(numpy.array(get_test_feature_vector())))\n",
    "  return classifier.predict(get_filtered_features_test())\n",
    "\n",
    "def evaluate_classifier(classifier):\n",
    "  test_predictions = make_predictions_with_classifier_on_test(classifier)\n",
    "  print sklearn.metrics.classification_report(get_test_labels(), test_predictions)\n",
    "\n",
    "def evaluate_classifier_for_user(classifier, user):\n",
    "  dataset = extract_secondlevel_dataset_from_user(user, True)\n",
    "  test_labels = get_labels_for_user(user)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#a= get_training_feature_vector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_selected_features_rfe():\n",
    "  classifier = sklearn.linear_model.SGDClassifier()\n",
    "  selector = sklearn.feature_selection.RFE(classifier, 10, step=1)\n",
    "  selector = selector.fit(numpy.array(get_training_feature_vector()), numpy.array(get_training_labels()))\n",
    "  return {\n",
    "    'n_features': selector.n_features_,\n",
    "    'support': map(int, selector.support_),\n",
    "    'ranking': map(int, selector.ranking_),\n",
    "  }\n",
    "  # return selector.ranking_\n",
    "\n",
    "def get_selected_features_rfecv():\n",
    "  classifier = sklearn.linear_model.SGDClassifier()\n",
    "  selector = sklearn.feature_selection.RFECV(classifier, step=1)\n",
    "  selector = selector.fit(numpy.array(get_training_feature_vector()), numpy.array(get_training_labels()))\n",
    "  return {\n",
    "    'n_features': selector.n_features_,\n",
    "    'support': map(int, selector.support_),\n",
    "    'ranking': map(int, selector.ranking_),\n",
    "  }\n",
    "\n",
    "def get_selected_features_chi2():\n",
    "  selector = sklearn.feature_selection.chi2(numpy.array(get_training_feature_vector()), numpy.array(get_training_labels()))\n",
    "  return {\n",
    "    'chi2': selector[0],\n",
    "    'pval': selector[1],\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "pickle_file = 'classifier_threefeatures_randomforest.pickle'\n",
    "if path.exists(pickle_file):\n",
    "  classifier = pickle.load(open(pickle_file))\n",
    "else:\n",
    "  classifier = get_classifier()\n",
    "  pickle.dump(classifier, open(pickle_file, 'w'), pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "evaluate_classifier(classifier)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def to_h2o_dataframe_csv_file(outpath, features, labels):\n",
    "  outfile = csv.writer(open(outpath, 'w'))\n",
    "  num_features = len(features[0])\n",
    "  outfile.writerow(['label'] + ['f'+str(i) for i in range(num_features)])\n",
    "  for idx in range(len(features)):\n",
    "    outfile.writerow([int(labels[idx])] + features[idx][0:3] + map(int, features[idx][3:]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# did this get trained on second-level data?\n",
    "train_dataset = sdir_path('h2odata_train_threefeatures_insession.csv')\n",
    "model_file = sdir_path('binclassifier_threefeatures_randomforest_insession.h2o')\n",
    "if path.exists(model_file):\n",
    "  pass\n",
    "  #classifier = h2o.load_model(model_file)\n",
    "else:\n",
    "  classifier = get_classifier()\n",
    "  #pickle.dump(classifier, open(pickle_file, 'w'), pickle.HIGHEST_PROTOCOL)\n",
    "  #evaluate_classifier(classifier)\n",
    "  print classifier\n",
    "  h2o.save_model(classifier, model_file)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "train_dataset = sdir_path('h2odata_train_threefeatures.csv')\n",
    "model_file = sdir_path('binclassifier_threefeatures_randomforest.h2o')\n",
    "if path.exists(model_file):\n",
    "  pass\n",
    "  #classifier = h2o.load_model(model_file)\n",
    "else:\n",
    "  classifier = get_classifier()\n",
    "  #pickle.dump(classifier, open(pickle_file, 'w'), pickle.HIGHEST_PROTOCOL)\n",
    "  #evaluate_classifier(classifier)\n",
    "  print classifier\n",
    "  h2o.save_model(classifier, model_file)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_classifier_second(model_name):\n",
    "  model_file = sdir_path(model_name)\n",
    "  if path.exists(model_file):\n",
    "    return\n",
    "  global train_dataset\n",
    "  train_dataset = sdir_path('catdata_train_second_shuffled.csv')\n",
    "  classifier = get_classifier()\n",
    "  print classifier\n",
    "  h2o.save_model(classifier, model_file)\n",
    "\n",
    "\n",
    "def train_classifier(model_name):\n",
    "  model_file = sdir_path(model_name)\n",
    "  if path.exists(model_file):\n",
    "    return\n",
    "  print model_name\n",
    "  global train_dataset\n",
    "  train_dataset = sdir_path('catdata_train_tensecond_shuffled.csv')\n",
    "  classifier = get_classifier()\n",
    "  print classifier\n",
    "  h2o.save_model(classifier, model_file)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_classifier_grid_search(model_name):\n",
    "  model_file = sdir_path(model_name)\n",
    "  if path.exists(model_file):\n",
    "    return\n",
    "  print model_name\n",
    "  global train_dataset\n",
    "  train_dataset = sdir_path('catdata_train_tensecond_shuffled.csv')\n",
    "  classifier = get_classifier()\n",
    "  print classifier\n",
    "  h2o.save_model(classifier, model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  '''\n",
    "  #hyper_parameters = {'ntrees':[10,50], 'max_depth':[20,10], 'binomial_double_trees':[True,False], 'nbins_cats': [1024,512]}\n",
    "  hyper_parameters = {'ntrees':[10,50], 'max_depth':[20,10]}\n",
    "  grid_search = h2o.grid.grid_search.H2OGridSearch(h2o.estimators.H2ORandomForestEstimator, hyper_params=hyper_parameters)\n",
    "  train_dataset = sdir_path('catdata_train_tensecond.csv')\n",
    "  training_data = h2o.import_file(train_dataset)\n",
    "  #test_data = h2o.import_file(train_dataset.replace('train', 'test'))\n",
    "  #classifier.train(x=training_data.columns[1:], y=training_data.columns[0], training_frame=training_data, validation_frame=test_data)\n",
    "  grid_search.train(x=training_data.columns[1:], y=training_data.columns[0], training_frame=training_data)\n",
    "  grid_search.show()\n",
    "  print grid_search.sort_by('F1', False)\n",
    "  best_model_id = grid_search.sort_by('F1', False)['Model Id'][0]\n",
    "  print grid_search.get_hyperparams(best_model_id)\n",
    "  best_model = h2o.get_model(best_model_id)\n",
    "  h2o.save_model(best_model, sdir_path('binclassifier_catfeatures_randomforest_gridsearch_v4.h2o'))\n",
    "  '''\n",
    "  is_second = len(sys.argv) > 1 and sys.argv[1] == 'second'\n",
    "  if is_second:\n",
    "    print 'doing second-level reconstruction'\n",
    "    train_dataset = sdir_path('catdata_train_second.csv')\n",
    "  else:\n",
    "    print 'doing tensecond-level reconstruction'\n",
    "    train_dataset = sdir_path('catdata_train_tensecond.csv')\n",
    "  training_data = h2o.import_file(train_dataset)\n",
    "  test_data = h2o.import_file(train_dataset.replace('train', 'test'))\n",
    "  for mtries,sample_rate in shuffled(list(itertools.product([1, 2, 3, 5, 6, 7], [0.50, 0.60, 0.70, 0.80, 0.90, 0.95, 1.0]))):\n",
    "    features_string = '_'.join(map(str, ['mtries', mtries, 'sample_rate', sample_rate]))\n",
    "    if is_second:\n",
    "      model_path = sdir_path('binclassifier_catfeatures_randomforest_second_v5_' + features_string + '.h2o')\n",
    "    else:\n",
    "      model_path = sdir_path('binclassifier_catfeatures_randomforest_v5_' + features_string + '.h2o')\n",
    "    if path.exists(model_path):\n",
    "      continue\n",
    "    print features_string\n",
    "    try:\n",
    "      classifier = h2o.estimators.H2ORandomForestEstimator(build_tree_one_node=True, mtries=mtries, sample_rate=sample_rate)\n",
    "      classifier.train(x=training_data.columns[1:], y=training_data.columns[0], training_frame=training_data, validation_frame=test_data)\n",
    "      h2o.save_model(classifier, model_path)\n",
    "      print classifier\n",
    "    except:\n",
    "      print traceback.format_exc()\n",
    "      continue\n",
    "  #classifier_algorithm = h2o.estimators.deeplearning.H2ODeepLearningEstimator\n",
    "  #train_classifier('binclassifier_catfeatures_deeplearning_v3.h2o')\n",
    "  \n",
    "  #classifier_algorithm = lambda: h2o.estimators.H2ORandomForestEstimator(ntrees=10, binomial_double_trees=True, build_tree_one_node=True)\n",
    "  #train_classifier('binclassifier_catfeatures_randomforest_v3.h2o')\n",
    "\n",
    "  #classifier_algorithm = lambda: h2o.estimators.H2OGradientBoostingEstimator(build_tree_one_node=True)\n",
    "  #train_classifier('binclassifier_catfeatures_gradientboost_v3.h2o')\n",
    "\n",
    "  # appears not to be a classification algorithm?\n",
    "  #classifier_algorithm = lambda: h2o.estimators.H2OGeneralizedLinearEstimator(solver='L_BFGS', family='binomial')\n",
    "  #train_classifier('binclassifier_catfeatures_generalizedlinear_lbfgs_v4.h2o')\n",
    "  \n",
    "  #classifier_algorithm = lambda: h2o.estimators.H2OGeneralizedLinearEstimator(solver='IRLSM', family='binomial')\n",
    "  #train_classifier('binclassifier_catfeatures_generalizedlinear_irlsm_v4.h2o')\n",
    "\n",
    "  #classifier_algorithm = lambda: h2o.estimators.H2ORandomForestEstimator(ntrees=10, binomial_double_trees=True, build_tree_one_node=True)\n",
    "  #train_classifier_second('binclassifier_catfeatures_randomforest_second.h2o')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "pickle_file = 'classifier_threefeatures_randomforest_v2.pickle'\n",
    "if path.exists(pickle_file):\n",
    "  classifier = pickle.load(open(pickle_file))\n",
    "else:\n",
    "  classifier = get_classifier()\n",
    "  pickle.dump(classifier, open(pickle_file, 'w'), pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "evaluate_classifier(classifier)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "pickle_file = 'classifier_allfeatures_randomforest.pickle'\n",
    "if path.exists(pickle_file):\n",
    "  classifier = pickle.load(open(pickle_file))\n",
    "else:\n",
    "  classifier = get_classifier()\n",
    "  pickle.dump(classifier, open(pickle_file, 'w'), pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "evaluate_classifier(classifier)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "pickle_file = 'classifier_allfeatures_randomforest_v2.pickle'\n",
    "if path.exists(pickle_file):\n",
    "  classifier = pickle.load(open(pickle_file))\n",
    "else:\n",
    "  classifier = get_classifier()\n",
    "  pickle.dump(classifier, open(pickle_file, 'w'), pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "evaluate_classifier(classifier)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#training_features = get_training_feature_vector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for line in training_features:\n",
    "#  if line[4] == 1:\n",
    "#    print line\n",
    "#    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dataset = extract_secondlevel_dataset_from_user(training_users[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#feature_vectors = dataset_to_feature_vectors(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for line in feature_vectors:\n",
    "#  if line[5] == 1:\n",
    "#    print line\n",
    "#    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#fromdomain_set = set(dataset['fromdomain'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print domain_to_id(top_n_domains_by_visits()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print [x for x in fromdomain_set if id_to_domain(x) == 'www.mturk.com']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "print 'get_selected_features_chi2'\n",
    "print datetime.datetime.fromtimestamp(time.time())\n",
    "print get_selected_features_chi2()\n",
    "print 'get_selected_features_rfe'\n",
    "print datetime.datetime.fromtimestamp(time.time())\n",
    "print get_selected_features_rfe()\n",
    "print 'get_selected_features_rfecv'\n",
    "print datetime.datetime.fromtimestamp(time.time())\n",
    "print get_selected_features_rfecv()\n",
    "#print get_selected_features_rfe()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print selector.support_\n",
    "#print selector.ranking_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#classifier = get_classifier()\n",
    "#evaluate_classifier(classifier)\n",
    "#evaluate_classifier(get_classifier())\n",
    "#classifier = get_classifier()\n",
    "#test_predictions = make_predictions_with_classifier_on_test(classifier)\n",
    "'''\n",
    "precision_all,recall_all,_ = sklearn.metrics.precision_recall_curve(get_test_labels(), test_predictions)\n",
    "\n",
    "best_f1 = 0.0\n",
    "precision = 0.0\n",
    "recall = 0.0\n",
    "for x,y in zip(precision_all,recall_all):\n",
    "  f1 = 2*(x*y)/(x+y)\n",
    "  if f1 > best_f1:\n",
    "    best_f1 = f1\n",
    "    precision = x\n",
    "    recall = y\n",
    "print best_f1, precision, recall\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

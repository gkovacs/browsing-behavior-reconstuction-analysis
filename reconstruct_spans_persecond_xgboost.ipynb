{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# noexport\n",
    "\n",
    "import os\n",
    "os.system('export_notebook reconstruct_spans_persecond_xgboost.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tmilib import *\n",
    "\n",
    "from reconstruct_focus_times_common import *\n",
    "from sorted_collection import SortedCollection\n",
    "from rescuetime_utils import *\n",
    "\n",
    "import sklearn\n",
    "import sklearn.svm\n",
    "import sklearn.linear_model\n",
    "import sklearn.ensemble\n",
    "import sklearn.naive_bayes\n",
    "import sklearn.tree\n",
    "import xgboost\n",
    "\n",
    "from math import log\n",
    "\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "import cPickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import traceback\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_users = get_training_users()\n",
    "test_users = get_test_users()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print domain_to_id('www.facebook.com')\n",
    "#print id_to_domain(29708)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def extract_secondlevel_dataset_from_users(users):\n",
    "  all_labels = []\n",
    "  all_sinceprev = []\n",
    "  all_tonext = []\n",
    "  all_fromdomain = []\n",
    "  all_todomain = []\n",
    "  for user in users:\n",
    "    data = extract_secondlevel_dataset_from_user(user)\n",
    "    all_labels.extend(data['label'])\n",
    "    all_sinceprev.extend(data['sinceprev'])\n",
    "    all_tonext.extend(data['tonext'])\n",
    "    all_fromdomain.extend(data['fromdomain'])\n",
    "    all_todomain.extend(data['todomain'])\n",
    "  return {\n",
    "    'label': all_labels,\n",
    "    'sinceprev': all_sinceprev,\n",
    "    'tonext': all_tonext,\n",
    "    'fromdomain': all_fromdomain,\n",
    "    'todomain': all_todomain,\n",
    "  }\n",
    "\n",
    "@jsonmemoized\n",
    "def extract_secondlevel_dataset_for_training():\n",
    "  return extract_secondlevel_dataset_from_users(training_users)\n",
    "\n",
    "@jsonmemoized\n",
    "def extract_secondlevel_dataset_for_test():\n",
    "  return extract_secondlevel_dataset_from_users(test_users)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_tensecondlevel_dataset_from_users(users):\n",
    "  all_labels = []\n",
    "  all_sinceprev = []\n",
    "  all_tonext = []\n",
    "  all_fromdomain = []\n",
    "  all_todomain = []\n",
    "  for user in users:\n",
    "    data = extract_secondlevel_dataset_from_user(user, True)\n",
    "    all_labels.extend(data['label'])\n",
    "    all_sinceprev.extend(data['sinceprev'])\n",
    "    all_tonext.extend(data['tonext'])\n",
    "    all_fromdomain.extend(data['fromdomain'])\n",
    "    all_todomain.extend(data['todomain'])\n",
    "  return {\n",
    "    'label': all_labels,\n",
    "    'sinceprev': all_sinceprev,\n",
    "    'tonext': all_tonext,\n",
    "    'fromdomain': all_fromdomain,\n",
    "    'todomain': all_todomain,\n",
    "  }\n",
    "\n",
    "@jsonmemoized\n",
    "def extract_tensecondlevel_dataset_for_training():\n",
    "  return extract_tensecondlevel_dataset_from_users(training_users)\n",
    "\n",
    "@jsonmemoized\n",
    "def extract_tensecondlevel_dataset_for_test():\n",
    "  return extract_tensecondlevel_dataset_from_users(test_users)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#a=extract_secondlevel_training_data_from_user(training_users[0])\n",
    "#print 'extracting dataset for training'\n",
    "#extract_secondlevel_dataset_for_training()\n",
    "#extract_tensecondlevel_dataset_for_training()\n",
    "#print 'extracting dataset for test'\n",
    "#extract_secondlevel_dataset_for_test()\n",
    "#extract_tensecondlevel_dataset_for_test()\n",
    "#print 'extraction done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print zipkeys({'a':[3,4,5], 'b':[6,7,8]}, 'a', 'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n@memoized\\ndef total_usage_of_domains_in_training():\\n  return sum_values_in_list_of_dict([get_domain_to_time_spent_for_user(user)for user in training_users])\\n\\n@memoized\\ndef top_n_domains_by_usage(n=10):\\n  domain_to_usage = total_usage_of_domains_in_training()\\n  return [x[0] for x in sorted(domain_to_usage.items(), key=itemgetter(1), reverse=True)[:n]]\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "@memoized\n",
    "def total_usage_of_domains_in_training():\n",
    "  return sum_values_in_list_of_dict([get_domain_to_time_spent_for_user(user)for user in training_users])\n",
    "\n",
    "@memoized\n",
    "def top_n_domains_by_usage(n=10):\n",
    "  domain_to_usage = total_usage_of_domains_in_training()\n",
    "  return [x[0] for x in sorted(domain_to_usage.items(), key=itemgetter(1), reverse=True)[:n]]\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print top_n_domains_by_visits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print top_n_domains_by_usage(10)\n",
    "#print top_n_domains_by_usage(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print domain_to_id('newtab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "def dataset_to_feature_vectors(dataset):\n",
    "  topdomains = numpy.array([domain_to_id(x) for x in top_n_domains_by_visits(20)])\n",
    "  num_features = 3 + 2*len(topdomains) + 2*len(get_rescuetime_productivity_levels())\n",
    "  output = [[0]*num_features for x in xrange(len(dataset['sinceprev']))]\n",
    "  #output = numpy.zeros((len(dataset['sinceprev']), num_features), dtype=object) # object instead of float, so we can have floats and ints\n",
    "  for idx,sinceprev,tonext,fromdomain,todomain in zipkeys_idx(dataset, 'sinceprev', 'tonext', 'fromdomain', 'todomain'):\n",
    "    cur = output[idx]\n",
    "    cur[0] = sinceprev\n",
    "    cur[1] = tonext\n",
    "    cur[2] = int(fromdomain == todomain)\n",
    "    feature_num = 3\n",
    "    for domain_idx,domain in enumerate(topdomains):\n",
    "      cur[feature_num+domain_idx] = int(fromdomain == domain)\n",
    "    feature_num += len(topdomains)\n",
    "    for domain_idx,domain in enumerate(topdomains):\n",
    "      cur[feature_num+domain_idx] = int(todomain == domain)\n",
    "    feature_num += len(topdomains)\n",
    "    fromdomain_name = id_to_domain(fromdomain)\n",
    "    todomain_name = id_to_domain(todomain)\n",
    "    fromdomain_productivity = domain_to_productivity(fromdomain_name)\n",
    "    todomain_productivity = domain_to_productivity(todomain_name)\n",
    "    for productivity_idx,productivity in enumerate(get_rescuetime_productivity_levels()):\n",
    "      cur[feature_num+productivity_idx] = int(fromdomain_productivity == productivity)\n",
    "    feature_num += len(get_rescuetime_productivity_levels())\n",
    "    for productivity_idx,productivity in enumerate(get_rescuetime_productivity_levels()):\n",
    "      cur[feature_num+productivity_idx] = int(todomain_productivity == productivity)\n",
    "    feature_num += len(get_rescuetime_productivity_levels())\n",
    "  return output\n",
    "'''\n",
    "\n",
    "def remove_cached_features():\n",
    "  os.remove('get_test_feature_vector.msgpack')\n",
    "  os.remove('get_training_feature_vector.msgpack')\n",
    "  #os.remove('get_test_labels.msgpack')\n",
    "  #os.remove('get_training_labels.msgpack')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#remove_cached_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n@msgpackmemoized\\ndef get_training_feature_vector():\\n  return dataset_to_feature_vectors(extract_tensecondlevel_dataset_for_training())\\n\\n@msgpackmemoized\\ndef get_training_labels():\\n  return extract_tensecondlevel_dataset_for_training()['label']\\n\\n@msgpackmemoized\\ndef get_test_feature_vector():\\n  return dataset_to_feature_vectors(extract_tensecondlevel_dataset_for_test())\\n\\n@msgpackmemoized\\ndef get_test_labels():\\n  return extract_tensecondlevel_dataset_for_test()['label']\\n\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "@msgpackmemoized\n",
    "def get_training_feature_vector():\n",
    "  return dataset_to_feature_vectors(extract_tensecondlevel_dataset_for_training())\n",
    "\n",
    "@msgpackmemoized\n",
    "def get_training_labels():\n",
    "  return extract_tensecondlevel_dataset_for_training()['label']\n",
    "\n",
    "@msgpackmemoized\n",
    "def get_test_feature_vector():\n",
    "  return dataset_to_feature_vectors(extract_tensecondlevel_dataset_for_test())\n",
    "\n",
    "@msgpackmemoized\n",
    "def get_test_labels():\n",
    "  return extract_tensecondlevel_dataset_for_test()['label']\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sklearn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-1cb7fb80f343>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     74\u001b[0m '''\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m \u001b[0mclassifier_algorithm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensemble\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdaBoostClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sklearn' is not defined"
     ]
    }
   ],
   "source": [
    "'''\n",
    "def train_classifier_on_data(training_data):\n",
    "  #classifier = sklearn.tree.DecisionTreeClassifier(max_depth=2) # .71 on test\n",
    "  #classifier = sklearn.tree.DecisionTreeClassifier(max_depth=1)\n",
    "  #classifier = sklearn.tree.DecisionTreeClassifier()\n",
    "  #classifier = sklearn.naive_bayes.GaussianNB() # .73 on test\n",
    "  #classifier = sklearn.svm.LinearSVC()\n",
    "  #classifier = sklearn.linear_model.SGDClassifier(class_weight='balanced')\n",
    "  classifier = sklearn.linear_model.SGDClassifier(loss='modified_huber') # .73 on test\n",
    "  classifier.fit(dataset_to_feature_vectors(training_data), numpy.array(training_data['label']))\n",
    "  return classifier\n",
    "\n",
    "def get_classifier():\n",
    "  return train_classifier_on_data(extract_tensecondlevel_dataset_for_training())\n",
    "'''\n",
    "\n",
    "global_feature_filter = '1'*53\n",
    "#global_feature_filter = '11100000000000000000000000000000000000000000000000000'\n",
    "\n",
    "def get_feature_filter():\n",
    "  return global_feature_filter\n",
    "  return ''.join(map(str, selected_features))\n",
    "\n",
    "is_second = len(sys.argv) > 1 and sys.argv[1] == 'second'\n",
    "\n",
    "@memoized\n",
    "def get_filtered_features_train():\n",
    "  selected_features = get_feature_filter()\n",
    "  if is_second:\n",
    "    return numpy.array(get_feature_vector_for_secondlevel_train(selected_features))\n",
    "  return numpy.array(get_feature_vector_for_tensecondlevel_train(selected_features))\n",
    "\n",
    "@memoized\n",
    "def get_filtered_features_test():\n",
    "  selected_features = get_feature_filter()\n",
    "  if is_second:\n",
    "    return numpy.array(get_feature_vector_for_secondlevel_test(selected_features))\n",
    "  return numpy.array(get_feature_vector_for_tensecondlevel_test(selected_features))\n",
    "\n",
    "@memoized\n",
    "def get_test_labels():\n",
    "  if is_second:\n",
    "    return numpy.array(get_labels_for_secondlevel_test())\n",
    "  return numpy.array(get_labels_for_tensecondlevel_test())\n",
    "\n",
    "@memoized\n",
    "def get_training_labels():\n",
    "  if is_second:\n",
    "    return numpy.array(get_labels_for_secondlevel_train())\n",
    "  return numpy.array(get_labels_for_tensecondlevel_train())\n",
    "\n",
    "#def get_labels_for_user(user):\n",
    "#  return get_tensecondlevel_activespan_labels_for_user(user)\n",
    "\n",
    "'''\n",
    "# we normally want to use the tensecond level ones\n",
    "def get_filtered_features_train():\n",
    "  selected_features = get_feature_filter()\n",
    "  return get_feature_vector_for_secondlevel_train(selected_features)\n",
    "\n",
    "def get_filtered_features_test():\n",
    "  selected_features = get_feature_filter()\n",
    "  return get_feature_vector_for_secondlevel_test(selected_features)\n",
    "\n",
    "def get_test_labels():\n",
    "  return get_labels_for_secondlevel_test()\n",
    "\n",
    "def get_training_labels():\n",
    "  return get_labels_for_secondlevel_train()\n",
    "\n",
    "def get_labels_for_user(user):\n",
    "  return get_secondlevel_activespan_labels_for_user(user)\n",
    "'''\n",
    "\n",
    "\n",
    "'''\n",
    "def filter_features(arr):\n",
    "  # from get_selected_features()\n",
    "  selected_features = get_feature_filter()\n",
    "  #selected_feature_idx = [i for i,x in enumerate(selected_features) if x]\n",
    "  #return arr[:,selected_feature_idx]\n",
    "  output = []\n",
    "  for line in arr:\n",
    "    output.append([line[i] for i,x in enumerate(selected_features) if x])\n",
    "    #output.append([x for i,x in enumerate(line) if selected_features[i]])\n",
    "  return output\n",
    "'''\n",
    "\n",
    "classifier_algorithm = xgboost.XGBClassifier\n",
    "xgboost_params = {}\n",
    "\n",
    "def get_classifier():\n",
    "  #classifier = sklearn.naive_bayes.GaussianNB()\n",
    "  #classifier = sklearn.linear_model.SGDClassifier(loss='modified_huber') # .73 on test\n",
    "  #classifier = sklearn.linear_model.SGDClassifier()\n",
    "  #classifier = sklearn.ensemble.RandomForestClassifier()\n",
    "  classifier = xgboost.XGBClassifier()\n",
    "  classifier.set_params(**xgboost_params)\n",
    "  #classifier.train()\n",
    "  #classifier = sklearn.ensemble.AdaBoostClassifier()\n",
    "  #classifier = sklearn.ensemble.GradientBoostingClassifier()\n",
    "  #classifier = classifier_algorithm()\n",
    "  classifier.fit(get_filtered_features_train(), get_training_labels())\n",
    "  return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#a=get_filtered_features_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#b=get_test_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print len(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print len(get_feature_filter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print len(get_training_feature_vector()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "def make_predictions_with_classifier_on_dataset(classifier, dataset):\n",
    "  return classifier.predict(dataset_to_feature_vectors(dataset))\n",
    "\n",
    "def make_proba_predictions_with_classifier_on_dataset(classifier, dataset):\n",
    "  return [x[1] for x in classifier.predict_proba(dataset_to_feature_vectors(dataset))]\n",
    "\n",
    "def evaluate_classifier(classifier):\n",
    "  test_predictions = make_predictions_with_classifier_on_dataset(classifier, test_data)\n",
    "  print sklearn.metrics.classification_report(test_data['label'], test_predictions)\n",
    "'''\n",
    "\n",
    "def make_predictions_with_classifier_on_test(classifier):\n",
    "  #return classifier.predict(filter_features(numpy.array(get_test_feature_vector())))\n",
    "  return classifier.predict(get_filtered_features_test())\n",
    "\n",
    "def make_predictions_with_classifier_on_train(classifier):\n",
    "  #return classifier.predict(filter_features(numpy.array(get_test_feature_vector())))\n",
    "  return classifier.predict(get_filtered_features_train())\n",
    "\n",
    "\n",
    "def evaluate_classifier(classifier):\n",
    "  test_predictions = make_predictions_with_classifier_on_test(classifier)\n",
    "  print sklearn.metrics.classification_report(get_test_labels(), test_predictions)\n",
    "\n",
    "def evaluate_classifier_train(classifier):\n",
    "  train_predictions = make_predictions_with_classifier_on_train(classifier)\n",
    "  print sklearn.metrics.classification_report(get_training_labels(), test_predictions)\n",
    "  \n",
    "def evaluate_classifier_for_user(classifier, user):\n",
    "  dataset = extract_secondlevel_dataset_from_user(user, True)\n",
    "  test_labels = get_labels_for_user(user)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#a= get_training_feature_vector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\npickle_file = 'classifier_threefeatures_randomforest.pickle'\\nif path.exists(pickle_file):\\n  classifier = pickle.load(open(pickle_file))\\nelse:\\n  classifier = get_classifier()\\n  pickle.dump(classifier, open(pickle_file, 'w'), pickle.HIGHEST_PROTOCOL)\\n\\nevaluate_classifier(classifier)\\n\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.83      0.81      0.82   1295568\n",
      "       True       0.83      0.84      0.83   1383871\n",
      "\n",
      "avg / total       0.83      0.83      0.83   2679439\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "global_feature_filter = '1'*53\n",
    "#classifier_algorithm = xgboost.XGBClassifier()\n",
    "xgboost_params = {}\n",
    "\n",
    "pickle_file = 'classifier_allfeatures_xgboost.pickle'\n",
    "if path.exists(pickle_file):\n",
    "  classifier = pickle.load(open(pickle_file))\n",
    "else:\n",
    "  classifier = get_classifier()\n",
    "  pickle.dump(classifier, open(pickle_file, 'w'), pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "evaluate_classifier(classifier)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "xgboost_params = {'objective': \"binary:logistic\"}\n",
    "\n",
    "pickle_file = 'classifier_allfeatures_xgboost_binary_logistic.pickle'\n",
    "if path.exists(pickle_file):\n",
    "  classifier = pickle.load(open(pickle_file))\n",
    "else:\n",
    "  classifier = get_classifier()\n",
    "  pickle.dump(classifier, open(pickle_file, 'w'), pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "evaluate_classifier(classifier)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgboost_params = {'objective': \"binary:logistic\"}\n",
    "for colsample_bytree in [1.0, 0.9, 0.8, 0.7, 0.6, 0.5]:\n",
    "  for subsample in shuffled([0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 1.0]):\n",
    "    xgboost_params['subsample'] = subsample\n",
    "    xgboost_params['colsample_bytree'] = colsample_bytree\n",
    "    features_string = '_'.join(map(str, ['subsample', subsample, 'colsample_bytree', colsample_bytree]))\n",
    "    if is_second:\n",
    "      classifier_filename = 'xgboost_second_allfeatures_' + features_string + '.pickle'\n",
    "    else:\n",
    "      classifier_filename = 'xgboost_tensecond_allfeatures_' + features_string + '.pickle'\n",
    "    if path.exists(classifier_filename):\n",
    "      continue\n",
    "    print classifier_filename\n",
    "    try:\n",
    "      classifier = get_classifier()\n",
    "      pickle.dump(classifier, open(classifier_filename, 'w'), pickle.HIGHEST_PROTOCOL)\n",
    "      evaluate_classifier(classifier)\n",
    "    except:\n",
    "      traceback.print_exc()\n",
    "      continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_vector_for_tensecondlevel_insession_train/11111111111111111111111111111111111111111111111111111.json\n",
      "feature_vector_for_tensecondlevel_insession_test/11111111111111111111111111111111111111111111111111111.json\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.84      0.85      0.85   1295568\n",
      "       True       0.86      0.85      0.85   1383871\n",
      "\n",
      "avg / total       0.85      0.85      0.85   2679439\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "xgboost_params = {'max_depth': 4, 'num_parallel_tree': 1000, 'subsample': 0.5, 'colsample_bytree': 0.5, 'nround': 1, 'objective': \"binary:logistic\"}\n",
    "\n",
    "pickle_file = 'classifier_allfeatures_xgboost_randomforest.pickle'\n",
    "if path.exists(pickle_file):\n",
    "  classifier = pickle.load(open(pickle_file))\n",
    "else:\n",
    "  classifier = get_classifier()\n",
    "  pickle.dump(classifier, open(pickle_file, 'w'), pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "evaluate_classifier(classifier)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\npickle_file = 'classifier_threefeatures_randomforest_v2.pickle'\\nif path.exists(pickle_file):\\n  classifier = pickle.load(open(pickle_file))\\nelse:\\n  classifier = get_classifier()\\n  pickle.dump(classifier, open(pickle_file, 'w'), pickle.HIGHEST_PROTOCOL)\\n\\nevaluate_classifier(classifier)\\n\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\npickle_file = 'classifier_allfeatures_randomforest.pickle'\\nif path.exists(pickle_file):\\n  classifier = pickle.load(open(pickle_file))\\nelse:\\n  classifier = get_classifier()\\n  pickle.dump(classifier, open(pickle_file, 'w'), pickle.HIGHEST_PROTOCOL)\\n\\nevaluate_classifier(classifier)\\n\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\npickle_file = 'classifier_allfeatures_randomforest_v2.pickle'\\nif path.exists(pickle_file):\\n  classifier = pickle.load(open(pickle_file))\\nelse:\\n  classifier = get_classifier()\\n  pickle.dump(classifier, open(pickle_file, 'w'), pickle.HIGHEST_PROTOCOL)\\n\\nevaluate_classifier(classifier)\\n\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nprint 'get_selected_features_chi2'\\nprint datetime.datetime.fromtimestamp(time.time())\\nprint get_selected_features_chi2()\\nprint 'get_selected_features_rfe'\\nprint datetime.datetime.fromtimestamp(time.time())\\nprint get_selected_features_rfe()\\nprint 'get_selected_features_rfecv'\\nprint datetime.datetime.fromtimestamp(time.time())\\nprint get_selected_features_rfecv()\\n#print get_selected_features_rfe()\\n\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprecision_all,recall_all,_ = sklearn.metrics.precision_recall_curve(get_test_labels(), test_predictions)\\n\\nbest_f1 = 0.0\\nprecision = 0.0\\nrecall = 0.0\\nfor x,y in zip(precision_all,recall_all):\\n  f1 = 2*(x*y)/(x+y)\\n  if f1 > best_f1:\\n    best_f1 = f1\\n    precision = x\\n    recall = y\\nprint best_f1, precision, recall\\n'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
